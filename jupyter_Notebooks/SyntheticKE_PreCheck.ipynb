{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4bb855-f17e-4b82-86c2-322db995c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubmed_parser as pp\n",
    "\n",
    "dicts_out = pp.parse_medline_xml('../data/pubmed_data/raw_data/44_abstracts_test.xml',\n",
    "                                 year_info_only=False,\n",
    "                                 nlm_category=False,\n",
    "                                 author_list=False,\n",
    "                                 reference_list=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0cc0846-7da3-41c6-9bde-ddd79d8d0fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABS_11604796.txt.done', 'ABS5_20841799.txt.done', 'ABS_29295283.txt.done', 'ABS_30306934.txt.done', 'ABS_28361157.txt.done', 'ABS_30871678.txt.done', 'ABS_29076113.txt.done', 'ABS5_22052898.txt.done', 'ABS_29433958.txt.done', 'ABS_19448882.txt.done', 'ABS_14664046.txt.done', 'ABS_24197356.txt.done', 'ABS_20501347.txt.done', 'ABS_29169646.txt.done', 'ABS_23304299.txt.done', 'ABS_29461493.txt.done', 'ABS_26133480.txt.done', 'ABS_18002176.txt.done', 'ABS_11198187.txt.done', 'ABS_18779083.txt.done', 'ABS_11418542.txt.done', 'ABS_26806717.txt.done', 'ABS5_12087117.txt.done', 'ABS_27401856.txt.done', 'ABS5_27301749.txt.done', 'ABS_28676255.txt.done', 'ABS_26539547.txt.done', 'ABS_22269224.txt.done', 'ABS_31438026.txt.done', 'ABS_31438019.txt.done', 'ABS_15360885.txt.done', 'ABS_17238374.txt.done', 'ABS_22326287.txt.done', 'ABS_23232759.txt.done', 'ABS_26590980.txt.done', 'ABS_22019377.txt.done', 'ABS5_23828174.txt.done', 'ABS_25109270.txt.done', 'ABS_26474836.txt.done', 'ABS_23523876.txt.done', 'ABS_31077222.txt.done', '.ipynb_checkpoints', 'ABS_24730353.txt.done']\n",
      "['11604796', '_20841799', '29295283', '30306934', '28361157', '30871678', '29076113', '_22052898', '29433958', '19448882', '14664046', '24197356', '20501347', '29169646', '23304299', '29461493', '26133480', '18002176', '11198187', '18779083', '11418542', '26806717', '_12087117', '27401856', '_27301749', '28676255', '26539547', '22269224', '31438026', '31438019', '15360885', '17238374', '22326287', '23232759', '26590980', '22019377', '_23828174', '25109270', '26474836', '23523876', '31077222', '24730353']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"/home/rgoli/MetaMap-src/data/gold_standard/AnnotationResults_Abstracts/done\")\n",
    "print(files)\n",
    "\n",
    "manual_pmids = [file.split('.')[0][4:] for file in files if file!='.ipynb_checkpoints']\n",
    "print(manual_pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c7567b-2990-424b-99ca-9d014bcf2dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug-ordering system', 'optimizing', 'guideline-based ordering system ', 'enable', 'prescription errors ', 'operates', 'background process ', 'triggered', 'rules ', 'directs', 'browsing', 'development', 'clinical practice guidelines', 'developed', 'disseminate', 'design', 'decision tree']\n"
     ]
    }
   ],
   "source": [
    "pmid_manual_kw={}\n",
    "for file in files:\n",
    "    if file=='.ipynb_checkpoints': continue\n",
    "    with open(\"/home/rgoli/MetaMap-src/data/gold_standard/AnnotationResults_Abstracts/done/\"+file,'r') as fp:\n",
    "        temp = fp.read().splitlines()\n",
    "        pmid_manual_kw[temp[0].strip()]=temp[1:]\n",
    "\n",
    "print(pmid_manual_kw['11604796'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa1aaec-8ae7-4f3c-bbbc-3ede00bd9333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(pmid_manual_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd32134-1a2a-401b-8d89-fa6f9e91b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.conda/envs/pytorch/lib/python3.8/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'en_core_sci_lg' (0.4.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.2.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ba460b-ea67-4ff3-ae1f-fa4f6c2398f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.local/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/rgoli/.local/lib/python3.8/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<scispacy.linking.EntityLinker at 0x15354280ff10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac04a27-7b05-436c-807a-0548a06f50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tit_arr = []\n",
    "# abs_arr = []\n",
    "# pmid_arr = []\n",
    "# for paper in dicts_out:\n",
    "#     tit_arr.append(paper['title'])\n",
    "#     abs_arr.append(paper['abstract'])\n",
    "#     pmid_arr.append(paper['pmid'])\n",
    "    \n",
    "# print(tit_arr[0:2])\n",
    "# print(abs_arr[0:2])\n",
    "# print(pmid_arr[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1622f8fc-7d7a-490a-bff3-8e8087343493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tit_kw = []\n",
    "# for doc in nlp.pipe(tit_arr):\n",
    "#     tit_kw.append(doc.ents)\n",
    "    \n",
    "# abs_kw = []\n",
    "# for doc in nlp.pipe(abs_arr):\n",
    "#     abs_kw.append(doc.ents)\n",
    "    \n",
    "# comb_kw = []\n",
    "# i=0\n",
    "# while i<len(abs_kw):\n",
    "#     comb_kw.append(Union(abs_kw[i], tit_kw[i]))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b459d493-577e-4f87-8c12-93fe87829ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tit_arr[0],\"\\n\")\n",
    "# print(abs_arr[0],\"\\n\")\n",
    "# print(list(set().union(abs_kw[0], tit_kw[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a883aea-35b0-49a2-b1ea-b1d35ce33622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11604796', '24730353']\n"
     ]
    }
   ],
   "source": [
    "comb_arr=[]\n",
    "pmid_arr =[]\n",
    "for paper in dicts_out:\n",
    "    comb_arr.append(paper['title']+' '+paper['abstract'])\n",
    "    pmid_arr.append(paper['pmid'])\n",
    "    \n",
    "print(pmid_arr[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5b94a2e-c0d5-4e08-b34d-7aac6c8ab92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTI ENTITY \n",
      ":  \n",
      "a  \n",
      "guideline-based ENTITY \n",
      "drug-ordering ENTITY \n",
      "system ENTITY \n",
      "for  \n",
      "primary ENTITY \n",
      "care ENTITY \n",
      ".  \n",
      "Existing  \n",
      "computer-based ENTITY \n",
      "ordering  \n",
      "systems ENTITY \n",
      "for  \n",
      "physicians ENTITY \n",
      "provide  \n",
      "effective ENTITY \n",
      "drug-centered ENTITY \n",
      "checks ENTITY \n",
      "but  \n",
      "offer  \n",
      "little  \n",
      "assistance ENTITY \n",
      "for  \n",
      "optimizing  \n",
      "the  \n",
      "overall  \n",
      "patient-centered ENTITY \n",
      "treatment ENTITY \n",
      "strategy ENTITY \n",
      ".  \n",
      "Evidence-based  \n",
      "clinical ENTITY \n",
      "practice ENTITY \n",
      "guidelines ENTITY \n",
      "have  \n",
      "been  \n",
      "developed  \n",
      "to  \n",
      "disseminate ENTITY \n",
      "state-of-the-art ENTITY \n",
      "information ENTITY \n",
      "concerning  \n",
      "treatment ENTITY \n",
      "strategy ENTITY \n",
      "but  \n",
      "these  \n",
      "guidelines ENTITY \n",
      "are  \n",
      "poorly  \n",
      "used  \n",
      "in  \n",
      "routine ENTITY \n",
      "practice ENTITY \n",
      ".  \n",
      "The  \n",
      "ASTI ENTITY \n",
      "project ENTITY \n",
      "aims  \n",
      "to  \n",
      "design ENTITY \n",
      "a  \n",
      "guideline-based ENTITY \n",
      "ordering ENTITY \n",
      "system ENTITY \n",
      "to  \n",
      "enable  \n",
      "general ENTITY \n",
      "practitioners ENTITY \n",
      "to  \n",
      "avoid  \n",
      "prescription ENTITY \n",
      "errors ENTITY \n",
      "and  \n",
      "to  \n",
      "improve  \n",
      "compliance ENTITY \n",
      "with  \n",
      "best  \n",
      "therapeutic ENTITY \n",
      "practices ENTITY \n",
      ".  \n",
      "The  \n",
      "\"  \n",
      "critic ENTITY \n",
      "mode ENTITY \n",
      "\"  \n",
      "operates  \n",
      "as  \n",
      "a  \n",
      "background  \n",
      "process  \n",
      "and  \n",
      "corrects  \n",
      "the  \n",
      "physician ENTITY \n",
      "'s ENTITY \n",
      "prescription ENTITY \n",
      "on  \n",
      "the  \n",
      "basis  \n",
      "of  \n",
      "automatically ENTITY \n",
      "triggered  \n",
      "elementary  \n",
      "rules  \n",
      "that  \n",
      "account  \n",
      "for  \n",
      "isolated ENTITY \n",
      "guideline ENTITY \n",
      "recommendations ENTITY \n",
      ".  \n",
      "The  \n",
      "\"  \n",
      "guided ENTITY \n",
      "mode ENTITY \n",
      "\"  \n",
      "directs  \n",
      "the  \n",
      "physician ENTITY \n",
      "to  \n",
      "the  \n",
      "best  \n",
      "treatment ENTITY \n",
      "by  \n",
      "browsing ENTITY \n",
      "a  \n",
      "comprehensive ENTITY \n",
      "guideline ENTITY \n",
      "knowledge  \n",
      "base  \n",
      "represented  \n",
      "as  \n",
      "a  \n",
      "decision ENTITY \n",
      "tree ENTITY \n",
      ".  \n",
      "A  \n",
      "first  \n",
      "prototype ENTITY \n",
      ",  \n",
      "applied  \n",
      "to  \n",
      "hypertension ENTITY \n",
      ",  \n",
      "is  \n",
      "currently  \n",
      "under  \n",
      "development ENTITY \n",
      ".  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.conda/envs/pytorch/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/home/rgoli/.conda/envs/pytorch/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp.pipe(comb_arr[:1]):\n",
    "    for ent in doc:\n",
    "        print(ent, ent.ent_type_,ent.ent_kb_id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ddc4713-75df-46d5-bc95-44feae74a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.conda/envs/pytorch/lib/python3.8/site-packages/scispacy/candidate_generation.py:284: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_neighbors[empty_vectors_boolean_flags] = numpy.array(neighbors)[:-1]\n",
      "/home/rgoli/.conda/envs/pytorch/lib/python3.8/site-packages/scispacy/candidate_generation.py:285: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  extended_distances[empty_vectors_boolean_flags] = numpy.array(distances)[:-1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "comb_kw=[]\n",
    "results=[]\n",
    "processed_texts=[]\n",
    "mappers=[]\n",
    "gs_mappers=[]\n",
    "\n",
    "j=0\n",
    "for doc in nlp.pipe(comb_arr[:1]):\n",
    "    text = [token.orth_ for token in doc if not token.is_punct | token.is_space] \n",
    "    text_len = len(text)\n",
    "    processed_texts.append(text)\n",
    "#     print(text,\"\\n\",\"-\"*50)\n",
    "\n",
    "    kws = [str(x) for x in doc.ents]\n",
    "    kws.sort(reverse=True, key=len)\n",
    "    kws = [x.split() for x in kws]\n",
    "#     print(kws,\"\\n\",\"-\"*50)\n",
    "\n",
    "    result = []\n",
    "    mapper = [0]*text_len\n",
    "    gs_mapper = [0]*text_len\n",
    "    for kw in kws:\n",
    "        kw_len=len(kw)\n",
    "        i=0\n",
    "        while i<text_len-kw_len:\n",
    "            if text[i:i+kw_len]==kw and mapper[i:i+kw_len]==[0]*kw_len:\n",
    "                mapper[i:i+kw_len]=[1]*kw_len\n",
    "                result.append([kw,[i,i+kw_len]])\n",
    "            i+=1\n",
    "            \n",
    "    gs_kws=pmid_manual_kw[pmid_arr[j]]\n",
    "    j+=1\n",
    "    gs_kws.sort(key=len, reverse=True)\n",
    "    gs_kws = [x.split() for x in gs_kws]\n",
    "#     print(gs_kws)\n",
    "    \n",
    "    for kw in gs_kws:\n",
    "        kw_len=len(kw)\n",
    "        i=0\n",
    "        while i<text_len-kw_len:\n",
    "            if text[i:i+kw_len]==kw and gs_mapper[i:i+kw_len]==[0]*kw_len:\n",
    "#                 print(kw, text[i:i+kw_len])\n",
    "                gs_mapper[i:i+kw_len]=[1]*kw_len\n",
    "            i+=1 \n",
    "            \n",
    "#     print(mapper)\n",
    "#     print(gs_mapper)\n",
    "\n",
    "#     print(mapper,\"\\n\",\"-\"*50)\n",
    "\n",
    "    results.append(result)\n",
    "    mappers.append(mapper)\n",
    "    gs_mappers.append(gs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c1d63b6-97f8-4d06-87e3-425e0ad398d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# text= processed_texts[0]\n",
    "# print(processed_texts[0])\n",
    "# print(pmid_manual_kw[pmid_arr[0]])\n",
    "# print(len(pmid_manual_kw[pmid_arr[0]]))\n",
    "# N=len(text)\n",
    "# while i<N:\n",
    "#     print(mappers[0][i],gs_mappers[0][i],text[i])\n",
    "#     i+=1\n",
    "# print(comb_arr[0],\"\\n\")\n",
    "# print(mappers[0])\n",
    "# print(gs_mappers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a9cb70-2663-41cf-925f-361e579f915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.6913668331464683\n",
      "Misclassification:\t0.30863316685353176\n",
      "Precision:\t0.3710513908533711\n",
      "Sensitivity/Recall:\t0.8138572905894519\n",
      "Specificity:\t0.6612923701916974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "TP, TN, FN, FP = 0, 0, 0, 0\n",
    "N = len(processed_texts)\n",
    "i=0\n",
    "while i<N:\n",
    "    matrix = confusion_matrix(gs_mappers[i],mappers[i], labels=[1,0])\n",
    "    TP += matrix[0][0]\n",
    "    FN += matrix[0][1]\n",
    "    FP += matrix[1][0]\n",
    "    TN += matrix[1][1]\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "misclassification = (FP+FN)/(TP+TN+FP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "\n",
    "print(\"Accuracy:\\t{}\\nMisclassification:\\t{}\\nPrecision:\\t{}\\nSensitivity/Recall:\\t{}\\nSpecificity:\\t{}\".format(accuracy,misclassification,\n",
    "                                                                                                            precision, sensitivity,\n",
    "                                                                                                            specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed8258a-91c7-481c-9f53-e6fb9194eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574 2668 360 5209\n"
     ]
    }
   ],
   "source": [
    "print(TP,FP,FN,TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c4d0dc-81ec-4b95-b35a-fcae6c769548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5097150259067358\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7f30c107-3aca-463a-8ef0-0dc7d5fdb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# while i<len(comb_kw):\n",
    "#     print(pmid_arr[i],'\\n')\n",
    "#     print(pmid_manual_kw[pmid_arr[i]],'\\n')\n",
    "#     print(comb_kw[i],'\\n')\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3887a68f-2980-475d-8d8e-b9d85486d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 11:41:01.955241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guideline-based ordering system', 'guideline-based drug-ordering system', 'computer-based ordering systems', 'asti project', 'asti', 'primary care', 'effective drug-centered checks', 'treatment strategy', 'routine practice', 'best treatment', 'prescription errors', 'little assistance', 'guidelines', 'best therapeutic practices', 'general practitioners', 'physicians', 'state-of-the-art information', 'prescription', 'critic mode', 'guideline recommendations']\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "\n",
    "# define the valid Part-of-Speeches to occur in the graph\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# define the grammar for selecting the keyphrase candidates\n",
    "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "# 1. create a PositionRank extractor.\n",
    "extractor = pke.unsupervised.PositionRank()\n",
    "\n",
    "# for doc in comb_arr:\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=comb_arr[0],\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the noun phrases up to 3 words as keyphrase candidates.\n",
    "extractor.candidate_selection(grammar=grammar,\n",
    "                              maximum_word_number=3)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk biaised with the position of the words\n",
    "#    in the document. In the graph, nodes are words (nouns and\n",
    "#    adjectives only) that are connected if they occur in a window of\n",
    "#    10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 20-highest scored candidates as keyphrases\n",
    "keyphrases = [x for x,y in extractor.get_n_best(n=20)] \n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b708b72a-cb4d-4622-b509-0fcc408ea684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "# grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "# result=[]\n",
    "# mappers=[]\n",
    "# gs_mappers=[]\n",
    "# for doc in comb_arr:\n",
    "#     extractor = pke.unsupervised.PositionRank()\n",
    "#     extractor.load_document(input=doc,language='en',normalization=None)\n",
    "#     extractor.candidate_selection(grammar=grammar,maximum_word_number=3)\n",
    "#     extractor.candidate_weighting(window=10,pos=pos)\n",
    "#     kws = [str(x) for x,y in extractor.get_n_best(n=20)] \n",
    "#     kws.sort(reverse=True, key=len)\n",
    "# #     kws = [x.split() for x in kws]\n",
    "#     print(kws)\n",
    "#     result = []\n",
    "#     mapper = [0]*text_len\n",
    "#     gs_mapper = [0]*text_len\n",
    "#     for kw in kws:\n",
    "#         kw_len=len(kw)\n",
    "#         i=0\n",
    "#         while i<text_len-kw_len:\n",
    "#             if text[i:i+kw_len]==kw and mapper[i:i+kw_len]==[0]*kw_len:\n",
    "#                 mapper[i:i+kw_len]=[1]*kw_len\n",
    "#                 result.append([kw,[i,i+kw_len]])\n",
    "#             i+=1\n",
    "            \n",
    "#     gs_kws=pmid_manual_kw[pmid_arr[j]]\n",
    "#     j+=1\n",
    "#     gs_kws.sort(key=len, reverse=True)\n",
    "#     gs_kws = [x.split() for x in gs_kws]\n",
    "\n",
    "#     for kw in gs_kws:\n",
    "#         kw_len=len(kw)\n",
    "#         i=0\n",
    "#         while i<text_len-kw_len:\n",
    "#             if text[i:i+kw_len]==kw and gs_mapper[i:i+kw_len]==[0]*kw_len:\n",
    "#                 gs_mapper[i:i+kw_len]=[1]*kw_len\n",
    "#             i+=1 \n",
    "\n",
    "#     results.append(result)\n",
    "#     mappers.append(mapper)\n",
    "#     gs_mappers.append(gs_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aab8f6d6-7b7a-4621-a6bc-69691287f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guideline-based drug-ordering system', 0.09509511549054267), ('asti', 0.07823207588239302), ('physicians', 0.06729787535294751), ('overall patient-centered treatment strategy', 0.06474731710691091), ('primary care', 0.049771410291627266), ('prescription errors', 0.0406724827691195), ('guidelines', 0.036677989985406445), ('critic mode', 0.03389492033146792), ('effective drug-centered checks', 0.03329143995283239), ('evidence-based clinical practice guidelines', 0.03328420904854418), ('little assistance', 0.030121655733911186), ('computer-based ordering systems', 0.028968465997458507), ('state-of-the-art information', 0.02598239262328236), ('best therapeutic practices', 0.02553702950572291), ('compliance', 0.024809850439883135), ('general practitioners', 0.024356422881381682), ('treatment strategy', 0.024348658985516584), ('routine practice', 0.022846921353418512), ('prescription', 0.021787162723204228), ('guideline-based ordering system', 0.021199318684860427)]\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a MultipartiteRank extractor.\n",
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=comb_arr[0])\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives, that do\n",
    "#    not contain punctuation marks or stopwords as candidates.\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "stoplist += stopwords.words('english')\n",
    "extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "# 4. build the Multipartite graph and rank candidates using random walk,\n",
    "#    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "#    threshold/method parameters.\n",
    "extractor.candidate_weighting(alpha=1.1,\n",
    "                              threshold=0.74,\n",
    "                              method='average')\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24862088-c58d-4ac0-b0bd-d884bffde6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug-ordering system', 'optimizing', 'guideline-based ordering system ', 'enable', 'prescription errors ', 'operates', 'background process ', 'triggered', 'rules ', 'directs', 'browsing', 'development', 'clinical practice guidelines', 'developed', 'disseminate', 'design', 'decision tree'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pmid_manual_kw[pmid_arr[0]],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d721c50-f60e-460f-b96b-c4b93b80a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('physicians', 0.0888403873735775), ('overall patient-centered treatment strategy', 0.08482354851434194), ('guideline-based drug-ordering system', 0.08100300358458583), ('prescription errors', 0.06326936466670863), ('guidelines', 0.05311235348860716), ('critic mode', 0.05247133728391051), ('asti', 0.04709528519972061), ('best therapeutic practices', 0.03618994629923039), ('effective drug-centered checks', 0.03581068598503181), ('compliance', 0.03502353958684092), ('basis', 0.03322368033962725), ('general practitioners', 0.03309114357952791), ('state-of-the-art information', 0.03242895254094424), ('evidence-based clinical practice guidelines', 0.031929811639632405), ('primary care', 0.03192936666229304), ('background process', 0.031223064785168078), ('routine practice', 0.0307813014512181), ('elementary rules', 0.03045161004750392), ('little assistance', 0.03044465824785106), ('decision tree', 0.030212655219831457)]\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a TopicRank extractor.\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=comb_arr[0])\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives, that do\n",
    "#    not contain punctuation marks or stopwords as candidates.\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "stoplist += stopwords.words('english')\n",
    "extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "# 4. build topics by grouping candidates with HAC (average linkage,\n",
    "#    threshold of 1/4 of shared stems). Weight the topics using random\n",
    "#    walk, and select the first occuring candidate from each topic.\n",
    "extractor.candidate_weighting(threshold=0.74, method='average')\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=20)\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29212ca7-4cd7-4233-9dec-4d28b76ef3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(\"*\"*150)\n",
    "    \n",
    "    \n",
    "#     for res in result:\n",
    "#         print(\"Position: {}\\t\\tWord: {}\".format(res[1],' '.join(res[0])))\n",
    "    \n",
    "#     i=0\n",
    "#     while i<text_len:\n",
    "#         print(mapper[i],text[i])\n",
    "# #         if mapper[i]==1:\n",
    "# #             print(\"\\033[91m {}\".format(text[i]),sep=' '),\n",
    "# #         else:\n",
    "# #             print(text[i],sep=' '),\n",
    "#         i+=1\n",
    "    \n",
    "#     result = []\n",
    "#     for kw in kws:\n",
    "#         kw_len = len(kw)\n",
    "#         if kw_len==1 and kw[0] in text:\n",
    "#             local_position = text.index(kw[0])\n",
    "#             result.append([text.pop(local_position),local_position])\n",
    "        \n",
    "#         i=0\n",
    "#         while i<text_len:\n",
    "#             if kw\n",
    "#             i+=1\n",
    "    \n",
    "#     print(\"=\"*100)\n",
    "#     print(len(result),\"/\",len(kws))\n",
    "#     print(result)\n",
    "            \n",
    "#     text=str(doc)\n",
    "   \n",
    "#     position =[]\n",
    "#     for kw in doc.ents:\n",
    "#         kw=str(kw)\n",
    "#         first_occurance = text.find(kw)\n",
    "#         last_position = position\n",
    "#         position = [(m.start(0), m.end(0)) for m in re.finditer(kw,text,re.IGNORECASE)]\n",
    "#         if len(position)>1 and len(last_position)==1:\n",
    "#             for cpos in position:\n",
    "#                 if cpos[0] > last_position[0][1]:\n",
    "#                     position = [cpos]\n",
    "#                     break\n",
    "#                 else:\n",
    "#                     position = [cpos]\n",
    "#         elif len(position)>1 and len(last_position)==0:\n",
    "#             position = [(first_occurance, first_occurance+len(kw))]\n",
    "#         elif len(position)==0:\n",
    "#             position =[(first_occurance, first_occurance+len(kw))]\n",
    "#         print(\"FirstOccurance: \",first_occurance,'\\tPosition: ',position,\"\\tWord: \",kw,\"\\Tag: \",doc[position[0][0]].tag_,\"\\tPOS: \",doc[position[0][0]].pos_)\n",
    "        \n",
    "#     comb_kw.append(set(map(str,doc.ents)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb309a2f-48ec-42b5-990e-40c43f94f2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017\n",
      "2035\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('GNA_Files/train.json','r') as f:\n",
    "    train_data = json.loads(f.read())\n",
    "    \n",
    "with open('GNA_Files/test.json','r') as f:\n",
    "    test_data = json.loads(f.read())\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb9200c-34b1-4524-9bfd-601dec4f240b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'abstract', 'journal', 'authors', 'pubdate', 'pmid', 'pmc', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'doi', 'references', 'delete', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'fullText', 'sentences'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da175b8e-2c3b-49cc-afef-c0e5acbb6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "train_corpus_sentences = []\n",
    "for x in train_data:\n",
    "    train_corpus_sentences.extend(x['sentences'])\n",
    "    data.extend([(x['pmid'],y) for y in x['sentences']])\n",
    "    \n",
    "test_corpus_sentences = []\n",
    "for x in test_data:\n",
    "    test_corpus_sentences.extend(x['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ee25a2-c686-480f-bdda-fdbb1330d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('34580243', ['continuous', 'cardiorespiratory', 'monitoring', 'is', 'a', 'dominant', 'source', 'of', 'predictive', 'signal', 'in', 'machine', 'learning', 'for', 'risk', 'stratification', 'and', 'clinical', 'decision', 'support'])\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6887060d-b70f-4c04-aa45-9f74f5b81917",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/#h2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8346bffc-9ffc-4c5a-b06f-2892b7994450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e595b59-49dc-4341-8ff1-49ac13659942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of length 5 tokens\n",
    "def create_seq(sent, seq_len = 5):\n",
    "    \n",
    "    sequences = []\n",
    "\n",
    "    # if the number of tokens in 'text' is greater than 5\n",
    "    if len(sent) > seq_len:\n",
    "        for i in range(seq_len, len(sent)):\n",
    "            # select sequence of tokens\n",
    "            seq = sent[i-seq_len:i+1]\n",
    "            # add to the list\n",
    "            sequences.append(\" \".join(seq))\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    # if the number of tokens in 'text' is less than or equal to 5\n",
    "    else:\n",
    "        return sent + ['<UNK>']*(len(sent)-seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c54d0d-3855-4cfa-af1f-6c6a8c46759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203435"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs = [create_seq(i) for i in train_corpus_sentences]\n",
    "\n",
    "# merge list-of-lists into a single list\n",
    "train_seqs = sum(train_seqs, [])\n",
    "\n",
    "# count of sequences\n",
    "len(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485b4491-e099-4868-8cf8-b3c75b1acef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there should be caution in assuming',\n",
       " 'should be caution in assuming that',\n",
       " 'be caution in assuming that predictive',\n",
       " 'caution in assuming that predictive models',\n",
       " 'in assuming that predictive models could',\n",
       " 'assuming that predictive models could improve',\n",
       " 'that predictive models could improve clinical',\n",
       " 'predictive models could improve clinical decision-making',\n",
       " 'models could improve clinical decision-making beaulieu-joneset',\n",
       " 'could improve clinical decision-making beaulieu-joneset al2021']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seqs[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb91857-35cb-4317-818d-a45635cda2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['continuous', 'cardiorespiratory', 'monitoring', 'is', 'a', 'dominant', 'source', 'of', 'predictive', 'signal', 'in', 'machine', 'learning', 'for', 'risk', 'stratification', 'and', 'clinical', 'decision', 'support'], ['beaulieu-jones', 'and', 'coworkers', 'propose', 'a', 'litmus', 'test', 'for', 'the', 'field', 'of', 'predictive', 'analytics-performance', 'improvements', 'must', 'be', 'demonstrated', 'to', 'be', 'the', 'result', 'of', 'non-clinician-initiated', 'data', 'otherwise', 'there', 'should', 'be', 'caution', 'in', 'assuming', 'that', 'predictive', 'models', 'could', 'improve', 'clinical', 'decision-making', 'beaulieu-joneset', 'al2021'], ['they', 'demonstrate', 'substantial', 'prognostic', 'information', 'in', 'unsorted', 'physician', 'orders', 'made', 'before', 'the', 'first', 'midnight', 'of', 'hospital', 'admission', 'and', 'we', 'are', 'persuaded', 'that', 'it', 'is', 'fair', 'to', 'ask-if', 'the', 'physician', 'thought', 'of', 'it', 'first', 'what', 'exactly', 'is', 'machine', 'learning', 'for', 'in-patient', 'risk', 'stratification', 'learning', 'about', 'while', 'we', 'want', 'predictive', 'analytics', 'to', 'represent', 'the', 'leading', 'indicators', 'of', 'a', 'patient', \"'s\", 'illness', 'does', 'it', 'instead', 'merely', 'reflect', 'the', 'lagging', 'indicators', 'of', 'clinicians', 'actions']]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus_sentences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26cb7bc8-16d4-4524-ac43-5ba43b0e60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(train_corpus_sentences, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34aede3-80f1-4d3f-96c7-6f2c5397218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in train_seqs:\n",
    "    x.append(\" \".join(s.split()[:-1]))\n",
    "    y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "106b88bf-2032-4afd-aa7b-17d7ec6e5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12071 0.805+/-0.056\n"
     ]
    }
   ],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {0:'<UNK>'}\n",
    "cnt = 1\n",
    "\n",
    "data = []\n",
    "for sent in train_corpus_sentences+test_corpus_sentences:\n",
    "    data.extend(sent)\n",
    "\n",
    "for w in set(data):\n",
    "    int2token[cnt] = w\n",
    "    cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}\n",
    "\n",
    "print(token2int[\"the\"], int2token[5177])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddcce9f5-b3c8-4c40-a527-b3a352e34953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26610"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f204caff-d076-4871-9bf9-9c653f8647a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integer_seq(seq):\n",
    "    return [token2int[w] for w in seq.split()]\n",
    "\n",
    "# convert text sequences to integer sequences\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]\n",
    "\n",
    "seq_len = 5\n",
    "x_int = [i for i in x_int if len(i)==seq_len]\n",
    "y_int = [i for i in y_int if len(i)==seq_len]\n",
    "\n",
    "# convert lists to numpy arrays\n",
    "x_int = np.array(x_int, dtype=\"int32\")\n",
    "y_int = np.array(y_int, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd6e9a4-2159-4143-b523-aba13c4d4ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14091 16438 21250 22784 18321]\n",
      "[16438 21250 22784 18321 24688]\n"
     ]
    }
   ],
   "source": [
    "print(x_int[0], y_int[0], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b580a809-4974-40f4-8992-b31a24474618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in y_int:\n",
    "    if len(x)!=5:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b0e52a-3bd1-4f71-8101-5792c2e7d349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16438, 21250, 22784, 18321, 24688], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b8eca2b-5d2e-441b-a8f5-f6bb8edd8794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14091, 16438, 21250, 22784, 18321], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b28156-124f-4413-aaf0-f77a79448b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "        x = arr_x[prv:n,:]\n",
    "        y = arr_y[prv:n,:]\n",
    "        prv = n\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68df01d6-f103-43c8-8762-8c5cbca9815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69953bda-2cb9-45df-8556-feac353c63be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(26610, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=26610, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18652e75-0351-43eb-898e-9cd45d4251c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            targets = targets.type(torch.LongTensor)\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "            \n",
    "            losses.append(loss.detach().item())\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\"Step: {}...\".format(counter),\"mean_loss : %0.2f, Perplexity : %0.2f\"%(np.mean(losses), np.exp(np.mean(losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e47594ae-d9ba-4507-98a4-5784d7f27bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 6338... mean_loss : 7.14, Perplexity : 1264.15\n",
      "Epoch: 2/20... Step: 12676... mean_loss : 6.40, Perplexity : 603.84\n",
      "Epoch: 3/20... Step: 19014... mean_loss : 6.00, Perplexity : 403.78\n",
      "Epoch: 4/20... Step: 25352... mean_loss : 5.70, Perplexity : 298.88\n",
      "Epoch: 5/20... Step: 31690... mean_loss : 5.50, Perplexity : 244.04\n",
      "Epoch: 6/20... Step: 38028... mean_loss : 5.34, Perplexity : 208.73\n",
      "Epoch: 7/20... Step: 44366... mean_loss : 5.21, Perplexity : 183.13\n",
      "Epoch: 8/20... Step: 50704... mean_loss : 5.10, Perplexity : 164.70\n",
      "Epoch: 9/20... Step: 57042... mean_loss : 5.02, Perplexity : 150.67\n",
      "Epoch: 10/20... Step: 63380... mean_loss : 4.94, Perplexity : 139.45\n",
      "Epoch: 11/20... Step: 69718... mean_loss : 4.87, Perplexity : 130.28\n",
      "Epoch: 12/20... Step: 76056... mean_loss : 4.81, Perplexity : 122.93\n",
      "Epoch: 13/20... Step: 82394... mean_loss : 4.77, Perplexity : 117.36\n",
      "Epoch: 14/20... Step: 88732... mean_loss : 4.72, Perplexity : 111.94\n",
      "Epoch: 15/20... Step: 95070... mean_loss : 4.68, Perplexity : 107.80\n",
      "Epoch: 16/20... Step: 101408... mean_loss : 4.64, Perplexity : 103.95\n",
      "Epoch: 17/20... Step: 107746... mean_loss : 4.61, Perplexity : 100.69\n",
      "Epoch: 18/20... Step: 114084... mean_loss : 4.58, Perplexity : 97.62\n",
      "Epoch: 19/20... Step: 120422... mean_loss : 4.55, Perplexity : 94.96\n",
      "Epoch: 20/20... Step: 126760... mean_loss : 4.53, Perplexity : 92.71\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc572cc6-8efa-4d6f-84bb-a9692e8e6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[token2int[tkn]]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # push to GPU\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top 3 values\n",
    "    top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "    # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "860334de-458c-45ec-9f43-a52ca67f19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "        \n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "        token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad0cdaac-5964-45d1-8bc8-d4f31851a02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is the leading to patients with a large risk for the first day of patients and'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59bce36a-e31e-4677-8727-aaf3c69a1c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the clinical data to develop the prognostic model to identify the economic cost-effectiveness and to assess'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"one of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71985a75-7c6d-4fb9-b3f6-2a64bcafc00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ordering system for a randomized review was used to compare a large number and economic studies of'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"ordering system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ae7e4a4-0d48-44a1-a904-e9dac5c21d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinical decision support systems for antimicrobial prescribing and patient safety and quality improvement and the basis for'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"clinical decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa4a6f50-1049-419d-9124-025c2ccbd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinical decision support systems for drug therapy and clinical data and a large basis for selecting and implementing a cdss for epilepsy and bleeding in patients and the the the clinical decision making for the clinical impact and the basis for the clinical pharmacist with no significant predictive values'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 45, prime = \"clinical decision support systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c196a469-5cae-4ae5-bd89-3f95e42d7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prescription errors of patients without the wells system for a decision support system cdss to identify patients and the second of the clinical trials with the number to be cost-effective for clinical practice in clinical care systems to identify the use of the cdss for a patient'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 45, prime = \"prescription errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fcf14e3-315f-422d-845c-fc648c707951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
    "# !mv 'GoogleNews-vectors-negative300.bin.gz?dl=1' GoogleNews-vectors-negative300.bin.gz\n",
    "# !gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548036ad-01bd-4235-bf22-0211e07ce956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, re\n",
    "import gensim\n",
    "\n",
    "def init_embeddings(vocab_size, embed_dim, unif):\n",
    "    return np.random.uniform(-unif, unif, (vocab_size, embed_dim))\n",
    "    \n",
    "\n",
    "class EmbeddingsReader:\n",
    "\n",
    "    @staticmethod\n",
    "    def from_text(filename, vocab, unif=0.25):\n",
    "        \n",
    "        with io.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.rstrip(\"\\n \")\n",
    "                values = line.split(\" \")\n",
    "\n",
    "                if i == 0:\n",
    "                    # fastText style\n",
    "                    if len(values) == 2:\n",
    "                        weight = init_embeddings(len(vocab), values[1], unif)\n",
    "                        continue\n",
    "                    # glove style\n",
    "                    else:\n",
    "                        weight = init_embeddings(len(vocab), len(values[1:]), unif)\n",
    "                word = values[0]\n",
    "                if word in vocab:\n",
    "                    vec = np.asarray(values[1:], dtype=np.float32)\n",
    "                    weight[vocab[word]] = vec\n",
    "        if '[PAD]' in vocab:\n",
    "            weight[vocab['[PAD]']] = 0.0\n",
    "        \n",
    "        embeddings = nn.Embedding(weight.shape[0], weight.shape[1])\n",
    "        embeddings.weight = nn.Parameter(torch.from_numpy(weight).float())\n",
    "        return embeddings, weight.shape[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_binary(filename, vocab, unif=0.25):\n",
    "        def read_word(f):\n",
    "\n",
    "            s = bytearray()\n",
    "            ch = f.read(1)\n",
    "\n",
    "            while ch != b' ':\n",
    "                s.extend(ch)\n",
    "                ch = f.read(1)\n",
    "            s = s.decode('utf-8')\n",
    "            # Only strip out normal space and \\n not other spaces which are words.\n",
    "            return s.strip(' \\n')\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        with io.open(filename, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            file_vocab_size, embed_dim = map(int, header.split())\n",
    "            weight = init_embeddings(len(vocab), embed_dim, unif)\n",
    "            if '[PAD]' in vocab:\n",
    "                weight[vocab['[PAD]']] = 0.0\n",
    "            width = 4 * embed_dim\n",
    "            for i in range(file_vocab_size):\n",
    "                word = read_word(f)\n",
    "                raw = f.read(width)\n",
    "                if word in vocab:\n",
    "                    vec = np.fromstring(raw, dtype=np.float32)\n",
    "                    weight[vocab[word]] = vec\n",
    "        embeddings = nn.Embedding(weight.shape[0], weight.shape[1])\n",
    "        embeddings.weight = nn.Parameter(torch.from_numpy(weight).float())\n",
    "        return embeddings, embed_dim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load('GNA_Files/wordvectors.kv', mmap='r')\n",
    "weights = torch.FloatTensor(model.vectors)\n",
    "class WordLSTM_V2(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GNA_Files/word2vec.bin\", token2int)\n",
    "        # self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GoogleNews-vectors-negative300.bin\", token2int) \n",
    "        #self.emb_layer = nn.Embedding.from_pretrained(weights) # nn.Embedding(vocab_size, 300)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(self.emb_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df83f123-c938-4beb-9c77-577d08a4add2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5247999.pbs02/ipykernel_2818706/2673353449.py:63: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  vec = np.fromstring(raw, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM_V2(\n",
      "  (emb_layer): Embedding(26610, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=26610, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net = WordLSTM_V2()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f76e3c7-ebb6-47e9-9440-7ac17687e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in net.named_parameters():\n",
    "#     print('name: ', name)\n",
    "#     print(type(param))\n",
    "#     print('param.shape: ', param.shape)\n",
    "#     print('param.requires_grad: ', param.requires_grad)\n",
    "#     print('=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9285a9e-d67a-4000-bee0-5bd7ba1fe07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 6338... mean_loss : 7.32, Perplexity : 1506.26\n",
      "Epoch: 2/20... Step: 12676... mean_loss : 6.88, Perplexity : 969.90\n",
      "Epoch: 3/20... Step: 19014... mean_loss : 6.42, Perplexity : 615.95\n",
      "Epoch: 4/20... Step: 25352... mean_loss : 5.96, Perplexity : 387.34\n",
      "Epoch: 5/20... Step: 31690... mean_loss : 5.65, Perplexity : 284.04\n",
      "Epoch: 6/20... Step: 38028... mean_loss : 5.45, Perplexity : 231.73\n",
      "Epoch: 7/20... Step: 44366... mean_loss : 5.30, Perplexity : 199.93\n",
      "Epoch: 8/20... Step: 50704... mean_loss : 5.18, Perplexity : 177.34\n",
      "Epoch: 9/20... Step: 57042... mean_loss : 5.08, Perplexity : 161.25\n",
      "Epoch: 10/20... Step: 63380... mean_loss : 4.99, Perplexity : 146.50\n",
      "Epoch: 11/20... Step: 69718... mean_loss : 4.91, Perplexity : 135.63\n",
      "Epoch: 12/20... Step: 76056... mean_loss : 4.84, Perplexity : 126.94\n",
      "Epoch: 13/20... Step: 82394... mean_loss : 4.78, Perplexity : 119.38\n",
      "Epoch: 14/20... Step: 88732... mean_loss : 4.73, Perplexity : 113.05\n",
      "Epoch: 15/20... Step: 95070... mean_loss : 4.69, Perplexity : 108.43\n",
      "Epoch: 16/20... Step: 101408... mean_loss : 4.65, Perplexity : 104.46\n",
      "Epoch: 17/20... Step: 107746... mean_loss : 4.62, Perplexity : 101.28\n",
      "Epoch: 18/20... Step: 114084... mean_loss : 4.59, Perplexity : 98.26\n",
      "Epoch: 19/20... Step: 120422... mean_loss : 4.56, Perplexity : 95.48\n",
      "Epoch: 20/20... Step: 126760... mean_loss : 4.53, Perplexity : 92.65\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74c4d10d-b5a5-4066-b6a3-17baa36dcd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical decision support systems cdss and a clinical trial and identify the effect and safety in terms on the most common medication warnings in patients who received a low risk for recurrence of recurrence in the primary care units and in a clinical decision making and a wide vte \n",
      "\n",
      "prescription errors and the use in order to predict the incidence of venous partial vte risk factors and a positive predictive value was used for further review and the development and development of an electronic health records ehrs for a patient and clinical outcomes and their health \n",
      "\n",
      "one of the most important and more common in terms of clinical decision rule for resuming extremity to the use of a cdss for predicting patients who had 18,008 provoking vte is a leading cause of patients who are likely with a valid model and the use and \n",
      "\n",
      "asthma management programs for primary care providers increasing adherence to asthma guidelines and clinical impact of the proposed framework is to develop the clinical trial is conducted to develop a decision tree algorithm and clinical decision rule for the detection and management strategies for the use and the use of clinical decision making and clinical practice and\n"
     ]
    }
   ],
   "source": [
    " ## Pre-trained CDSS Word2Vec\n",
    "print(sample(net, 45, prime = \"clinical decision support systems\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"prescription errors\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"one of the\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"asthma management programs for primary care providers increasing adherence to asthma guidelines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69a36847-2967-4ddd-b71f-cfbddaf83138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinical decision support systems cdss are used to identify patients with a large patient in patients who were randomly assigned to either the final and superficial vein thrombosis uedvt n < 0.001 and bleeding management and the clinical outcomes of the clinical practice and management of patients who had'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 45, prime = \"clinical decision support systems\") ## Pre-trained Google-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62811489-a634-4907-bf60-2cea0de3fdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prescription errors and to identify the first step for clinical outcomes of patients with a decision rule for resuming oac prescriptions and a decision support system cdss for resuming oac therapy in patients and the cockcroft-gault y had recurrent vte vs. uesvt had a high risk for'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 45, prime = \"prescription errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15d49113-0153-4eb9-8e84-55fab2e68f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the most common risk of the number and clinical outcomes in clinical medicine in clinical medicine'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"one of the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6075c0f-f50f-43ce-8d1a-0efb64dfe99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asthma management programs for primary care providers increasing adherence to asthma guidelines in the emergency care applied for a clinical decision making in the management process in'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"asthma management programs for primary care providers increasing adherence to asthma guidelines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ba1f1fc-52ee-4d87-9234-fb07396dea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'although a number of studies have evaluated the effectiveness of computerized decision-support systems cdss there is lack of data on user perspectives barriers and satisfaction of clinical decision rules for a clinical decision aid with an electronic health record-based'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 15, prime = \"although a number of studies have evaluated the effectiveness of computerized decision-support systems cdss there is lack of data on user perspectives barriers and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2239395-6182-479b-9095-1f3437dc88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'GNA_Files/BILM_google_emb.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0c030-592d-40d6-914b-27c9236d77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_new = WordLSTM_V2()\n",
    "# model_new.load_state_dict(torch.load('GNA_Files/BILM_google_emb.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70a6481a-a670-476a-86ce-b8ae53d26aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bi-Directional LSTM\n",
    "class WordLSTM_V3(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GNA_Files/word2vec.bin\", token2int)\n",
    "        # self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GoogleNews-vectors-negative300.bin\", token2int) \n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(self.emb_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden*2, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden*2) \n",
    "        out = out.reshape(-1, self.n_hidden*2) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90d44416-4dbf-4427-847b-36351b6d5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5247999.pbs02/ipykernel_2818706/2673353449.py:63: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  vec = np.fromstring(raw, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM_V3(\n",
      "  (emb_layer): Embedding(26610, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=26610, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net3 = WordLSTM_V3()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net3.cuda()\n",
    "\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94454e86-da2b-429c-9fc5-21696449324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 6338... mean_loss : 7.49, Perplexity : 1791.99\n",
      "Epoch: 2/20... Step: 12676... mean_loss : 6.85, Perplexity : 947.29\n",
      "Epoch: 3/20... Step: 19014... mean_loss : 4.55, Perplexity : 94.66\n",
      "Epoch: 4/20... Step: 25352... mean_loss : 2.93, Perplexity : 18.79\n",
      "Epoch: 5/20... Step: 31690... mean_loss : 2.24, Perplexity : 9.37\n",
      "Epoch: 6/20... Step: 38028... mean_loss : 1.84, Perplexity : 6.33\n",
      "Epoch: 7/20... Step: 44366... mean_loss : 1.59, Perplexity : 4.91\n",
      "Epoch: 8/20... Step: 50704... mean_loss : 1.46, Perplexity : 4.32\n",
      "Epoch: 9/20... Step: 57042... mean_loss : 1.35, Perplexity : 3.84\n",
      "Epoch: 10/20... Step: 63380... mean_loss : 1.28, Perplexity : 3.60\n",
      "Epoch: 11/20... Step: 69718... mean_loss : 1.23, Perplexity : 3.43\n",
      "Epoch: 12/20... Step: 76056... mean_loss : 1.17, Perplexity : 3.22\n",
      "Epoch: 13/20... Step: 82394... mean_loss : 1.13, Perplexity : 3.10\n",
      "Epoch: 14/20... Step: 88732... mean_loss : 1.12, Perplexity : 3.06\n",
      "Epoch: 15/20... Step: 95070... mean_loss : 1.08, Perplexity : 2.94\n",
      "Epoch: 16/20... Step: 101408... mean_loss : 1.07, Perplexity : 2.90\n",
      "Epoch: 17/20... Step: 107746... mean_loss : 1.04, Perplexity : 2.83\n",
      "Epoch: 18/20... Step: 114084... mean_loss : 1.03, Perplexity : 2.81\n",
      "Epoch: 19/20... Step: 120422... mean_loss : 1.01, Perplexity : 2.74\n",
      "Epoch: 20/20... Step: 126760... mean_loss : 1.00, Perplexity : 2.72\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net3, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8f9587f-6a17-47de-bccc-a5c0bc79dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical decision support systems for improving quality of life and quality and care of the clinical impact on the use on health outcomes of clinical decision support system cdss to assess whether cdss to assess the impact on quality of care in a clinical decision rule to assess if \n",
      "\n",
      "prescription errors and a clinical decision support systems for the management and treatment of patients with uedvt in the united states management in patients with the use of a computerized clinical decision support systems cdss and workflow and patient care to identify patients with the aim and \n",
      "\n",
      "one of the study was to determine a wide range of concept of the clinical decision rule in a chinese setting in a clinical decision making to identify the use of a decision rule to assess whether cdss for patients with recurrence and economic models and thus reduce \n",
      "\n",
      "ordering system and a clinical decision tree and clinical practice guidelines cpgs in the emergency departments in clinical trials in this paper we present the system was used to identify and validate the prognostic models for the development of an emr-embedded clinical knowledge and clinical and economic \n",
      "\n",
      "asthma management programs for primary care providers increasing adherence to asthma guidelines and regulatory challenges for a computerized decision making in a clinical decision rule for the management of a computerized decision rule for the use and to improve the effectiveness in the emergency department in a large range of clinical decision rules for predicting patients who\n"
     ]
    }
   ],
   "source": [
    " ## Pre-trained CDSS Word2Vec + Bi-Directional LM with 4 LSTM layers\n",
    "print(sample(net, 45, prime = \"clinical decision support systems\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"prescription errors\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"one of the\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"ordering system\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"asthma management programs for primary care providers increasing adherence to asthma guidelines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "55255e19-3b84-46e2-b4c5-21635ec3152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinical decision support systems support to use of cdss of the in the in patient management these management these study in cdss in outcomes with terms with terms as context of terms in terms from a from a had the reduction the reduction a protocol for content by challenges'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net3, 45, prime = \"clinical decision support systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "023a4da6-0678-4ce4-b0c9-7b9ae0cf95a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ordering system ordering system trigger system to it will review will be will been will most used more evaluated also identified and order the identified the effect on context we aim and effectiveness of lack of a for two by a clinical an decision medical impact support'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net3, 45, prime = \"ordering system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb24d2-4f38-4c0d-b249-089b6e428ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86516e25-d200-4ad4-9a81-b0e255331606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM_V4(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=1, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GNA_Files/word2vec.bin\", token2int)\n",
    "        #self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(self.emb_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden*2, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden*2) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a068988-eb29-4d64-aee1-b2e73600c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5250410.pbs02/ipykernel_3573651/2673353449.py:63: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  vec = np.fromstring(raw, dtype=np.float32)\n",
      "/home/rgoli/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "WordLSTM_V4(\n",
      "  (emb_layer): Embedding(26610, 300)\n",
      "  (lstm): LSTM(300, 256, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=26610, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net4 = WordLSTM_V4()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net4.cuda()\n",
    "\n",
    "print(net4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d9923c-1c4c-46b0-a900-313b0900c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 6338... mean_loss : 4.37, Perplexity : 79.16\n",
      "Epoch: 2/20... Step: 12676... mean_loss : 2.55, Perplexity : 12.84\n",
      "Epoch: 3/20... Step: 19014... mean_loss : 1.90, Perplexity : 6.68\n",
      "Epoch: 4/20... Step: 25352... mean_loss : 1.54, Perplexity : 4.67\n",
      "Epoch: 5/20... Step: 31690... mean_loss : 1.33, Perplexity : 3.79\n",
      "Epoch: 6/20... Step: 38028... mean_loss : 1.20, Perplexity : 3.33\n",
      "Epoch: 7/20... Step: 44366... mean_loss : 1.11, Perplexity : 3.04\n",
      "Epoch: 8/20... Step: 50704... mean_loss : 1.04, Perplexity : 2.83\n",
      "Epoch: 9/20... Step: 57042... mean_loss : 0.98, Perplexity : 2.67\n",
      "Epoch: 10/20... Step: 63380... mean_loss : 0.93, Perplexity : 2.54\n",
      "Epoch: 11/20... Step: 69718... mean_loss : 0.89, Perplexity : 2.43\n",
      "Epoch: 12/20... Step: 76056... mean_loss : 0.85, Perplexity : 2.34\n",
      "Epoch: 13/20... Step: 82394... mean_loss : 0.81, Perplexity : 2.26\n",
      "Epoch: 14/20... Step: 88732... mean_loss : 0.78, Perplexity : 2.19\n",
      "Epoch: 15/20... Step: 95070... mean_loss : 0.76, Perplexity : 2.13\n",
      "Epoch: 16/20... Step: 101408... mean_loss : 0.73, Perplexity : 2.08\n",
      "Epoch: 17/20... Step: 107746... mean_loss : 0.71, Perplexity : 2.03\n",
      "Epoch: 18/20... Step: 114084... mean_loss : 0.69, Perplexity : 1.98\n",
      "Epoch: 19/20... Step: 120422... mean_loss : 0.67, Perplexity : 1.95\n",
      "Epoch: 20/20... Step: 126760... mean_loss : 0.65, Perplexity : 1.91\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(net4, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4fb13e-b01a-44f9-b9e4-522029a460fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical decision support systems and to improve treatment management treatment for treatment of treatment of treatment of type-ii alerts a case study case case case case test characteristics fell to the index the index the index order to alert to provide to provide relevant emerging tools that tools that \n",
      "\n",
      "prescription errors in errors in errors in errors in errors in clinical practice and workflow in the icu the icu the hospital the icu database by a university hospital from june 2014 and 2014 to december 2007 n = n and 4 and 4 of the patient \n",
      "\n",
      "one of the frequency and bleeding and in other in other medical centers medical centers in the first the first the development and validation study validation and two main two main two main predictors and results and the ewd identifying clinical guidelines were guidelines for the development and \n",
      "\n",
      "ordering system ordering system ordering system ordering through the electronic health record to ensure to extract the patient 's health state health state and state dependent property dependent on single biomolecular single cardiovascular magnetic resonance imaging scans imaging adjusted for adjusted for the patient the patients with \n",
      "\n",
      "asthma management programs for primary care providers increasing adherence to asthma guidelines asthma management and management with chronic patients chronic patients receiving an insulin bolus insulin therapy for insulin for insulin dosing error in a downstream alert downstream alert burden alert alert and providers are provided in provided without provided without fear and symptoms of vt when\n"
     ]
    }
   ],
   "source": [
    " ## Pre-trained CDSS Word2Vec + Bi-Directional LM with 1 LSTM layers\n",
    "print(sample(net4, 45, prime = \"clinical decision support systems\"),\"\\n\")\n",
    "print(sample(net4, 45, prime = \"prescription errors\"),\"\\n\")\n",
    "print(sample(net4, 45, prime = \"one of the\"),\"\\n\")\n",
    "print(sample(net4, 45, prime = \"ordering system\"),\"\\n\")\n",
    "print(sample(net4, 45, prime = \"asthma management programs for primary care providers increasing adherence to asthma guidelines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1bdc7dc-b290-4812-8f16-25326ef6796d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clinical decision support systems cdsss are cdsss in cdsss in cdsss in which a system has system in the clinical setting a setting in the management and management system which system which is which is an effective form in the context and development implementation of cds for clinical decision-making'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net4, 45, prime = \"clinical decision support systems\") ## On-the-fly word Embeddings with Bi-Directional LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dfbc3870-230e-4368-ae54-895b06832c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ordering system ordering system ordering in cpoe into cpoe towards the cancer a less a modest the modest single capacity of probabilistic case 65 paper of time especially software 40 the knowledge-based pharmacy despite countries of countries especially pharmacy especially of target interview target interview and interview'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net4, 45, prime = \"ordering system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5aeaf-c570-4e3a-a88d-bcf8dc6874ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_fasttext_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a27e58d-08f5-4db6-adbc-b8c55ef1d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLSTM_V5(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer, self.emb_dim = EmbeddingsReader.from_binary(\"GNA_Files/fastText.bin\", token2int)\n",
    "        #self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(self.emb_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden*2, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden*2) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers*2, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98b312c7-fa10-49fe-be02-c89d5b332d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5231816.pbs02/ipykernel_1801255/2673353449.py:63: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  vec = np.fromstring(raw, dtype=np.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM_V5(\n",
      "  (emb_layer): Embedding(26610, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=26610, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "net5 = WordLSTM_V5()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net5.cuda()\n",
    "\n",
    "print(net5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ae230b9-393a-487b-8756-342149d53143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 6338...\n",
      "Epoch: 2/20... Step: 12676...\n",
      "Epoch: 3/20... Step: 19014...\n",
      "Epoch: 4/20... Step: 25352...\n",
      "Epoch: 5/20... Step: 31690...\n",
      "Epoch: 6/20... Step: 38028...\n",
      "Epoch: 7/20... Step: 44366...\n",
      "Epoch: 8/20... Step: 50704...\n",
      "Epoch: 9/20... Step: 57042...\n",
      "Epoch: 10/20... Step: 63380...\n",
      "Epoch: 11/20... Step: 69718...\n",
      "Epoch: 12/20... Step: 76056...\n",
      "Epoch: 13/20... Step: 82394...\n",
      "Epoch: 14/20... Step: 88732...\n",
      "Epoch: 15/20... Step: 95070...\n",
      "Epoch: 16/20... Step: 101408...\n",
      "Epoch: 17/20... Step: 107746...\n",
      "Epoch: 18/20... Step: 114084...\n",
      "Epoch: 19/20... Step: 120422...\n",
      "Epoch: 20/20... Step: 126760...\n"
     ]
    }
   ],
   "source": [
    "# train the model - FastText pre-trained CDSS + Bi-LSTM 4 Layers\n",
    "train(net5, batch_size = 32, epochs=20, print_every=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc116817-d586-4978-a330-091a3877e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical decision support systems cdss can improve the efficiency and use of cdss for the field was used in a three-year database was undertaken using the final logistic regressions were selected using a clinical model and a centralized ionized calcium is the most important factor in patients with type \n",
      "\n",
      "prescription errors were identified in this paper was to develop and evaluate the economic cost-effectiveness of a cdss in order with the aim of a clinical decision support systems cdsss are a first ionized d-dimer model and a proof of concept we used an electronic clinical trials \n",
      "\n",
      "one of the development of a computerized clinical decision rules to improve patient care in a clinical setting and the cockcroft-gault standard in a large effect to identify and validate the clinical impact of the actual clinical practice is the most common type 1 and a number and \n",
      "\n",
      "ordering system to improve the efficiency and use of cdss in the field for patients and older of the seniors medication therapy is a common cause for the most important causes in patients with a low risk and the clinical utility of a cdss in a hospital \n",
      "\n",
      "asthma management programs for primary care providers increasing adherence to asthma guidelines in the united kingdom clinical trials were included to assess the effect on clinical practice and clinical decision support systems cdsss can be implemented in a three-year index period and were used for the first of a cdss in patients with a clinical pharmacist and\n"
     ]
    }
   ],
   "source": [
    " ## Pre-trained CDSS FastText + Bi-Directional LM with 4 LSTM layers\n",
    "print(sample(net, 45, prime = \"clinical decision support systems\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"prescription errors\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"one of the\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"ordering system\"),\"\\n\")\n",
    "print(sample(net, 45, prime = \"asthma management programs for primary care providers increasing adherence to asthma guidelines\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae184b-9b0f-4f05-bfac-fb8c4c048bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c73745-f8b9-4053-a795-b06faa74dfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4602d4-20e7-42e2-bdfe-7652377a6753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830a4b7-4dcd-415f-b6b2-ae54c5245d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7b878-a994-4b2e-80d2-19b1be30cd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0b4137a9-6294-427a-85b9-b8d3ad00e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "modelW2V = gensim.models.Word2Vec.load('GNA_Files/word2vec.embedding')\n",
    "# path = get_tmpfile(\"GNA_Files/wordvectors.kv\")\n",
    "modelW2V.wv.save(\"GNA_Files/wordvectors.kv\")\n",
    "\n",
    "wv = KeyedVectors.load(\"GNA_Files/wordvectors.kv\", mmap='r')\n",
    "# >>> vector = wv['computer']\n",
    "# weights = torch.FloatTensor(modelW2V.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "178c7ac3-6a18-47a8-8602-93df67411679",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e40cc3c0-60b2-4ad6-95e0-30af95a3b7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15755, 300])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297828ce-c584-4144-a482-1c2160efaad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d516c61-a0a8-4172-aec5-0fd478f2fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "# modelFT = FT_gensim.load('GNA_Files/fastText.embedding')\n",
    "\n",
    "modelFT = FT_gensim(size=300)\n",
    "modelFT.build_vocab(sentences=train_corpus_sentences)\n",
    "modelFT.train(\n",
    "    sentences=train_corpus_sentences, epochs=modelFT.epochs,\n",
    "    total_examples=modelFT.corpus_count, total_words=modelFT.corpus_total_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08da0040-2fe7-40d9-85ef-7f870d5bfdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12023"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a453d768-42d4-4aa1-a130-a1b4e2ffb222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n"
     ]
    }
   ],
   "source": [
    "vocab = modelFT.wv.vocab\n",
    "print(vocab['continuous'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8433f6d-1244-453b-a7ee-c1bc1b37efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f825c526-5cbe-4e2a-9d9b-f92c7e00c560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clinic', 0.9903262853622437),\n",
       " ('supporting', 0.9902169704437256),\n",
       " ('supports', 0.9857264757156372),\n",
       " ('making', 0.983847975730896),\n",
       " ('support', 0.9830864667892456),\n",
       " ('compute', 0.9816253781318665),\n",
       " ('integrate', 0.9809657335281372),\n",
       " ('technical', 0.9803340435028076),\n",
       " ('systems', 0.979055643081665),\n",
       " ('integral', 0.977682888507843)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFT.wv.most_similar(positive=['clinical'], topn = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53729178-4cad-4daa-b0f1-1763c175663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = []\n",
    "for x in train_corpus_sentences:\n",
    "    total_data.extend(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd5c3a11-b29e-48e5-a061-583b8063c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continuous', 'cardiorespiratory', 'monitoring', 'is', 'a', 'dominant', 'source', 'of', 'predictive', 'signal', 'in', 'machine', 'learning', 'for', 'risk', 'stratification', 'and', 'clinical', 'decision', 'support', 'beaulieu-jones', 'and', 'coworkers', 'propose', 'a', 'litmus', 'test', 'for', 'the', 'field', 'of', 'predictive', 'analytics-performance', 'improvements', 'must', 'be', 'demonstrated', 'to', 'be', 'the', 'result', 'of', 'non-clinician-initiated', 'data', 'otherwise', 'there', 'should', 'be', 'caution', 'in', 'assuming', 'that', 'predictive', 'models', 'could', 'improve', 'clinical', 'decision-making', 'beaulieu-joneset', 'al2021', 'they', 'demonstrate', 'substantial', 'prognostic', 'information', 'in', 'unsorted', 'physician', 'orders', 'made', 'before', 'the', 'first', 'midnight', 'of', 'hospital', 'admission', 'and', 'we', 'are', 'persuaded', 'that', 'it', 'is', 'fair', 'to', 'ask-if', 'the', 'physician', 'thought', 'of', 'it', 'first', 'what', 'exactly', 'is', 'machine', 'learning', 'for', 'in-patient', 'risk', 'stratification', 'learning', 'about', 'while', 'we', 'want', 'predictive', 'analytics', 'to', 'represent', 'the', 'leading', 'indicators', 'of', 'a', 'patient', \"'s\", 'illness', 'does', 'it', 'instead', 'merely', 'reflect', 'the', 'lagging', 'indicators', 'of', 'clinicians', 'actions', 'we', 'propose', 'that', 'continuous', 'cardiorespiratory', \"monitoring-'routine\", 'telemetry', 'data', 'in', 'beaulieu-jones', 'terms-represents', 'the', 'most', 'valuable', 'non-clinician-initiated', 'predictive', 'signal', 'present', 'in', 'patient', 'data', 'and', 'the', 'value', 'added', 'to', 'patient', 'care', 'justifies', 'the', 'efforts', 'and', 'expense', 'required', 'here', 'we', 'present', 'a', 'clinical', 'and', 'a', 'physiological', 'point', 'of', 'view', 'to', 'support', 'our', 'contention', 'the', 'clinical', 'pharmacist', \"'s\", 'role', 'in', 'enhancing', 'the', 'relevance', 'of', 'a', 'clinical', 'decision', 'support', 'system', 'background', 'clinical', 'decision', 'support', 'systems', 'cdsss']\n"
     ]
    }
   ],
   "source": [
    "print(total_data[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18e0cfd3-16a2-4b0c-ac95-e271587f8eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cc = list(set(total_data))\n",
    "word2index = {'<unk>': 0}\n",
    "for vo in vocab_cc:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6d21c19-bb55-4a72-9516-1278cccbc2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15755"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fc139c1-c156-4c9b-8723-22b017f58ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<unk>\"], seq))\n",
    "    return LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb8dd6c9-2bca-40d8-b5f3-3837163e303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "207273cd-261f-4e7c-8720-12f06f6f7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9262,  1809, 10032,  3989, 10668,  2373,  4493, 15710,  6035,  5917,\n",
       "         1103,  9198,  7785,  6642, 11958,  7816,  1540,  5392,  3037, 11699],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(train_corpus_sentences[0], word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f102dfc0-49c5-429e-9659-0274f294aeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9262"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['continuous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebc45907-fe91-49bb-a663-83ac2140c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ptb_dataset(corpus, word2index=None):\n",
    "    corpus = flatten([co + ['</s>'] for co in corpus])\n",
    "    \n",
    "    if word2index == None:\n",
    "        vocab = list(set(corpus))\n",
    "        word2index = {'<unk>': 0}\n",
    "        for vo in vocab:\n",
    "            if word2index.get(vo) is None:\n",
    "                word2index[vo] = len(word2index)\n",
    "    \n",
    "    return prepare_sequence(corpus, word2index), word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68b86328-f213-4def-a172-292953a5aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed code from https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).contiguous()\n",
    "    if USE_CUDA:\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f95a2b2-a748-4cd2-93bf-814f18b9677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(data, seq_length):\n",
    "     for i in range(0, data.size(1) - seq_length, seq_length):\n",
    "        inputs = Variable(data[:, i: i + seq_length])\n",
    "        targets = Variable(data[:, (i + 1): (i + 1) + seq_length].contiguous())\n",
    "        yield (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5274429-1f63-4996-a8ec-431a6c03ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, word2index = prepare_ptb_dataset(train_corpus_sentences)\n",
    "test_data, _ = prepare_ptb_dataset(test_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e3fb46b-45c3-469d-8fe4-eda38140519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9263,  1809, 10033,  ...,  3037, 11700,  4632], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "685eea9a-ca7a-47c1-ae7f-f5cadbcd294a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15757"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fecd6834-5b00-4bfb-88b1-c3236ad847c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78028cf0-a7c8-44eb-8e5c-c2456435bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module): \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.5):\n",
    "\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.embed.weight = nn.init.xavier_uniform(self.embed.weight)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = Variable(torch.zeros(self.n_layers,batch_size,self.hidden_size))\n",
    "        context = Variable(torch.zeros(self.n_layers,batch_size,self.hidden_size))\n",
    "        return (hidden.cuda(), context.cuda()) if USE_CUDA else (hidden, context)\n",
    "    \n",
    "    def detach_hidden(self, hiddens):\n",
    "        return tuple([hidden.detach() for hidden in hiddens])\n",
    "    \n",
    "    def forward(self, inputs, hidden, is_training=False): \n",
    "\n",
    "        embeds = self.embed(inputs)\n",
    "        if is_training:\n",
    "            embeds = self.dropout(embeds)\n",
    "        out,hidden = self.rnn(embeds, hidden)\n",
    "        return self.linear(out.contiguous().view(out.size(0) * out.size(1), -1)), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f7ddf80-6a8a-481f-9f21-d897939b5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 1024\n",
    "NUM_LAYER = 1\n",
    "LR = 0.01\n",
    "SEQ_LENGTH = 30 # for bptt\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 40\n",
    "RESCHEDULED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92008c9c-cd8d-4489-bf39-255176f45c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(train_data, BATCH_SIZE)\n",
    "test_data = batchify(test_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "357435ad-632c-4d60-b82d-dd46d347ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/3030259437.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.embed.weight = nn.init.xavier_uniform(self.embed.weight)\n",
      "/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/3030259437.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(len(word2index), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYER, 0.5)\n",
    "model.init_weight() \n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e3f1f02a-8ad5-43f9-a79f-602f987ef065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/346701661.py:14: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 0.5) # gradient clipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/40] mean_loss : 6.83, Perplexity : 926.73\n",
      "[05/40] mean_loss : 4.41, Perplexity : 82.11\n",
      "[10/40] mean_loss : 3.03, Perplexity : 20.76\n",
      "[15/40] mean_loss : 2.21, Perplexity : 9.11\n",
      "[20/40] mean_loss : 1.69, Perplexity : 5.44\n",
      "[25/40] mean_loss : 1.39, Perplexity : 4.00\n",
      "[30/40] mean_loss : 1.20, Perplexity : 3.32\n",
      "[35/40] mean_loss : 1.07, Perplexity : 2.91\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i,batch in enumerate(getBatch(train_data, SEQ_LENGTH)):\n",
    "        inputs, targets = batch\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        preds, hidden = model(inputs, hidden, True)\n",
    "\n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5) # gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%5==0:\n",
    "        print(\"[%02d/%d] mean_loss : %0.2f, Perplexity : %0.2f\" % (epoch,EPOCH, np.mean(losses), np.exp(np.mean(losses))))\n",
    "        losses = []\n",
    "        \n",
    "    # learning rate anealing\n",
    "    # You can use http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\n",
    "    if RESCHEDULED == False and epoch == EPOCH//2:\n",
    "        LR *= 0.1\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        RESCHEDULED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e2dbb5b-573a-49a8-a2ad-70d64ef47843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perpelexity : 15.89\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "hidden = model.init_hidden(BATCH_SIZE)\n",
    "for batch in getBatch(train_data, SEQ_LENGTH):\n",
    "    inputs,targets = batch\n",
    "        \n",
    "    hidden = model.detach_hidden(hidden)\n",
    "    model.zero_grad()\n",
    "    preds, hidden = model(inputs, hidden)\n",
    "    total_loss += inputs.size(1) * loss_function(preds, targets.view(-1)).data\n",
    "\n",
    "total_loss = total_loss.item()/train_data.size(1)\n",
    "print(\"Train Perpelexity : %5.2f\" % (np.exp(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6a70c9e-c1ae-467b-89eb-34154e781b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/687456838.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_ptb_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/1349116509.py\u001b[0m in \u001b[0;36mprepare_ptb_dataset\u001b[0;34m(corpus, word2index)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/1074193046.py\u001b[0m in \u001b[0;36mprepare_sequence\u001b[0;34m(seq, to_index)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mto_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<unk>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "test_data, _ = prepare_ptb_dataset(test_corpus_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e3937-64fc-456b-9a55-f897dabbe970",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = batchify(test_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87496445-3777-4e38-ab2f-e36c734363e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/1546808304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_scratch/pbs.5214499.pbs02/ipykernel_4092084/3030259437.py\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetach_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "hidden = model.init_hidden(BATCH_SIZE)\n",
    "for batch in getBatch(test_data, SEQ_LENGTH):\n",
    "    inputs,targets = batch\n",
    "        \n",
    "    hidden = model.detach_hidden(hidden)\n",
    "    model.zero_grad()\n",
    "    preds, hidden = model(inputs, hidden)\n",
    "    total_loss += inputs.size(1) * loss_function(preds, targets.view(-1)).data\n",
    "\n",
    "total_loss = total_loss.item()/test_data.size(1)\n",
    "print(\"Train Perpelexity : %5.2f\" % (np.exp(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6643919-84a4-4108-bd41-0926c88f36e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b328f32-e9c0-4867-a55e-e5bd0363fd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], device='cuda:0', size=(20, 0), dtype=torch.int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18468b83-b161-4a1d-b10c-049e2e1e467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urinary',\n",
       " 'colorimetric',\n",
       " 'sensor',\n",
       " 'array',\n",
       " 'and',\n",
       " 'algorithm',\n",
       " 'to',\n",
       " 'distinguish',\n",
       " 'kawasaki',\n",
       " 'disease',\n",
       " 'from',\n",
       " 'other',\n",
       " 'febrile',\n",
       " 'illnesses']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7da82-8312-408a-a8a2-b62621fb5a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

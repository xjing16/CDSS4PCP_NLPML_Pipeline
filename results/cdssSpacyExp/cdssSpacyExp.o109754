[38;5;2mâœ” Auto-filled config with all values[0m
[38;5;2mâœ” Saved config[0m
config.cfg
You can now add your data and train your pipeline:
python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 18:30:25,178] [INFO] Set up nlp object from config
[2022-09-10 18:30:25,187] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 18:30:25,187] [INFO] Resuming training for: ['ner']
[2022-09-10 18:30:25,196] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 18:30:32,697] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 18:30:46,008] [INFO] Created vocabulary
[2022-09-10 18:30:47,482] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 18:30:49,071] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.78   68.34   66.37   70.42    0.68
 28     200           0.00  64685.79   70.12   73.96   66.67    0.70
 57     400           0.00  43982.89   70.05   74.08   66.43    0.70
 85     600           0.00  29445.39   68.17   73.12   63.85    0.68
114     800           0.00  20595.66   66.33   71.35   61.97    0.66
142    1000           0.00  14678.00   65.41   70.16   61.27    0.65
171    1200           0.00  10323.73   65.84   69.63   62.44    0.66
200    1400           0.00   7674.50   64.60   68.32   61.27    0.65
228    1600           0.00   5785.68   64.01   68.17   60.33    0.64
257    1800           0.00   4435.06   64.48   69.57   60.09    0.64
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 18:45:52,592] [INFO] Set up nlp object from config
[2022-09-10 18:45:52,602] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 18:45:52,602] [INFO] Resuming training for: ['ner']
[2022-09-10 18:45:52,610] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 18:46:00,001] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 18:46:13,072] [INFO] Created vocabulary
[2022-09-10 18:46:14,520] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 18:46:16,106] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.07   48.20   43.87   53.47    0.48
 28     200           0.00  54740.03   47.70   46.41   49.05    0.48
 57     400           0.00  40053.17   49.25   49.89   48.63    0.49
 85     600           0.00  27826.23   50.95   50.95   50.95    0.51
114     800           0.00  19578.71   49.17   48.66   49.68    0.49
142    1000           0.00  14573.35   50.10   49.90   50.32    0.50
171    1200           0.00  10502.60   50.57   49.80   51.37    0.51
200    1400           0.00   7952.56   50.05   49.39   50.74    0.50
228    1600           0.00   5826.32   49.90   49.08   50.74    0.50
257    1800           0.00   4416.09   48.90   48.55   49.26    0.49
285    2000           0.00   3482.45   49.63   49.38   49.89    0.50
314    2200           0.00   2840.97   49.28   48.47   50.11    0.49
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 19:02:22,714] [INFO] Set up nlp object from config
[2022-09-10 19:02:22,723] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 19:02:22,723] [INFO] Resuming training for: ['ner']
[2022-09-10 19:02:22,732] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 19:02:30,161] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 19:02:43,341] [INFO] Created vocabulary
[2022-09-10 19:02:44,803] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 19:02:46,405] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    200.10   75.28   75.35   75.20    0.75
 28     200           0.00  53828.40   74.95   81.52   69.35    0.75
 57     400           0.00  36912.14   74.48   80.71   69.15    0.74
 85     600           0.00  24065.27   73.55   80.72   67.54    0.74
114     800           0.00  16168.30   73.28   78.70   68.55    0.73
142    1000           0.00  11141.97   72.84   78.24   68.15    0.73
171    1200           0.00   7796.76   72.77   79.15   67.34    0.73
200    1400           0.00   5697.65   72.81   78.69   67.74    0.73
228    1600           0.00   4214.24   72.41   77.78   67.74    0.72
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 19:15:25,993] [INFO] Set up nlp object from config
[2022-09-10 19:15:26,002] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 19:15:26,002] [INFO] Resuming training for: ['ner']
[2022-09-10 19:15:26,011] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 19:15:33,385] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 19:15:46,503] [INFO] Created vocabulary
[2022-09-10 19:15:47,954] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 19:15:51,193] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.07   70.38   66.30   75.00    0.70
 28     200           0.00  59262.15   70.65   69.54   71.78    0.71
 57     400           0.00  41415.35   72.01   71.74   72.28    0.72
 85     600           0.00  27536.46   72.82   71.43   74.26    0.73
114     800           0.00  19011.28   71.55   70.60   72.52    0.72
142    1000           0.00  13537.49   71.26   69.58   73.02    0.71
171    1200           0.00   9596.75   70.41   67.97   73.02    0.70
200    1400           0.00   7112.76   69.96   68.24   71.78    0.70
228    1600           0.00   5255.20   69.55   67.68   71.53    0.70
257    1800           0.00   4104.46   69.02   67.78   70.30    0.69
285    2000           0.00   3373.03   69.23   67.29   71.29    0.69
314    2200           0.00   2685.41   69.39   67.37   71.53    0.69
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 19:33:51,793] [INFO] Set up nlp object from config
[2022-09-10 19:33:51,803] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 19:33:51,803] [INFO] Resuming training for: ['ner']
[2022-09-10 19:33:51,812] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 19:34:00,397] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 19:34:15,077] [INFO] Created vocabulary
[2022-09-10 19:34:16,579] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 19:34:18,196] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.78   60.55   59.31   61.85    0.61
 28     200           0.00  50849.88   61.47   64.52   58.69    0.61
 57     400           0.00  35554.30   62.29   66.08   58.92    0.62
 85     600           0.00  23713.07   62.78   66.33   59.59    0.63
114     800           0.00  16166.83   63.92   66.67   61.40    0.64
142    1000           0.00  11093.94   63.76   66.58   61.17    0.64
171    1200           0.00   7895.67   62.91   65.53   60.50    0.63
200    1400           0.00   5691.06   63.15   65.77   60.72    0.63
228    1600           0.00   4125.26   62.34   64.05   60.72    0.62
257    1800           0.00   3182.61   61.95   63.72   60.27    0.62
285    2000           0.00   2563.38   62.06   64.48   59.82    0.62
314    2200           0.00   2085.44   61.18   63.88   58.69    0.61
342    2400           0.00   1683.44   61.61   63.77   59.59    0.62
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 19:55:36,261] [INFO] Set up nlp object from config
[2022-09-10 19:55:36,272] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 19:55:36,272] [INFO] Resuming training for: ['ner']
[2022-09-10 19:55:36,282] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 19:55:44,718] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 19:55:59,111] [INFO] Created vocabulary
[2022-09-10 19:56:00,577] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 19:56:02,220] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.01   57.20   50.53   65.89    0.57
 28     200           0.00  60855.18   57.36   53.75   61.48    0.57
 57     400           0.00  42724.63   59.47   57.17   61.95    0.59
 85     600           0.00  28737.32   60.80   57.14   64.97    0.61
114     800           0.00  20132.87   60.22   56.11   64.97    0.60
142    1000           0.00  14641.17   59.51   55.40   64.27    0.60
171    1200           0.00  10159.44   59.68   55.36   64.73    0.60
200    1400           0.00   7422.95   59.51   55.40   64.27    0.60
228    1600           0.00   5490.44   59.02   54.40   64.50    0.59
257    1800           0.00   4233.63   57.91   53.66   62.88    0.58
285    2000           0.00   3322.37   58.24   54.08   63.11    0.58
314    2200           0.00   2625.45   58.16   54.45   62.41    0.58
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 20:14:29,398] [INFO] Set up nlp object from config
[2022-09-10 20:14:29,407] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 20:14:29,407] [INFO] Resuming training for: ['ner']
[2022-09-10 20:14:29,416] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 20:14:37,838] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 20:14:52,332] [INFO] Created vocabulary
[2022-09-10 20:14:53,814] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 20:14:55,467] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.00   56.05   50.94   62.30    0.56
 28     200           0.00  55361.99   59.03   57.92   60.18    0.59
 57     400           0.00  38401.80   60.21   58.88   61.59    0.60
 85     600           0.00  25594.91   60.15   58.31   62.12    0.60
114     800           0.00  17424.75   60.17   58.66   61.77    0.60
142    1000           0.00  12112.19   60.34   58.82   61.95    0.60
171    1200           0.00   8772.30   60.69   59.16   62.30    0.61
200    1400           0.00   6193.89   59.93   58.04   61.95    0.60
228    1600           0.00   4577.91   60.26   58.50   62.12    0.60
257    1800           0.00   3455.25   60.29   58.72   61.95    0.60
285    2000           0.00   2862.11   60.37   58.10   62.83    0.60
314    2200           0.00   2282.15   59.95   58.39   61.59    0.60
342    2400           0.00   2012.25   59.47   57.64   61.42    0.59
371    2600           0.00   1622.33   60.53   58.39   62.83    0.61
400    2800           0.00   1374.21   59.85   57.73   62.12    0.60
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 20:37:29,270] [INFO] Set up nlp object from config
[2022-09-10 20:37:29,279] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 20:37:29,279] [INFO] Resuming training for: ['ner']
[2022-09-10 20:37:29,288] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 20:37:37,717] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 20:37:52,070] [INFO] Created vocabulary
[2022-09-10 20:37:53,537] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 20:37:55,141] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     98.75   66.38   63.73   69.26    0.66
 28     200           0.00  61558.25   66.98   70.68   63.65    0.67
 57     400           0.00  43186.88   66.54   71.52   62.21    0.67
 85     600           0.00  28019.79   66.99   71.14   63.29    0.67
114     800           0.00  19129.39   66.54   69.70   63.65    0.67
142    1000           0.00  13262.02   65.98   68.28   63.83    0.66
171    1200           0.00   9135.92   65.97   69.11   63.11    0.66
200    1400           0.00   6431.12   65.09   68.46   62.03    0.65
228    1600           0.00   4827.41   65.79   68.49   63.29    0.66
257    1800           0.00   3692.54   65.40   69.15   62.03    0.65
285    2000           0.00   2902.40   65.53   68.79   62.57    0.66
314    2200           0.00   2327.65   64.88   69.12   61.12    0.65
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 20:55:29,459] [INFO] Set up nlp object from config
[2022-09-10 20:55:29,468] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 20:55:29,468] [INFO] Resuming training for: ['ner']
[2022-09-10 20:55:29,477] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 20:55:37,892] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 20:55:52,240] [INFO] Created vocabulary
[2022-09-10 20:55:53,724] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 20:55:55,401] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    113.26   73.84   69.98   78.14    0.74
 28     200           0.00  60676.00   75.40   73.97   76.88    0.75
 57     400           0.00  44013.34   75.73   74.61   76.88    0.76
 85     600           0.00  30186.19   76.48   75.22   77.78    0.76
114     800           0.00  21446.90   76.69   75.30   78.14    0.77
142    1000           0.00  15576.97   76.42   74.79   78.14    0.76
171    1200           0.00  11404.59   76.27   74.32   78.32    0.76
200    1400           0.00   8296.49   75.57   73.64   77.60    0.76
228    1600           0.00   6147.56   75.35   73.55   77.24    0.75
257    1800           0.00   4663.54   74.87   72.48   77.42    0.75
285    2000           0.00   3674.73   75.02   73.41   76.70    0.75
314    2200           0.00   2964.03   74.18   73.46   74.91    0.74
342    2400           0.00   2414.25   74.12   72.84   75.45    0.74
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 21:15:50,615] [INFO] Set up nlp object from config
[2022-09-10 21:15:50,626] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 21:15:50,626] [INFO] Resuming training for: ['ner']
[2022-09-10 21:15:50,636] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 21:15:59,406] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 21:16:14,436] [INFO] Created vocabulary
[2022-09-10 21:16:15,960] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 21:16:17,581] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.07   54.44   51.33   57.94    0.54
 28     200           0.00  62416.18   54.49   54.26   54.72    0.54
 57     400           0.00  44635.50   56.59   56.96   56.22    0.57
 85     600           0.00  29947.88   57.23   57.17   57.30    0.57
114     800           0.00  20898.11   56.90   56.30   57.51    0.57
142    1000           0.00  15084.72   56.11   55.58   56.65    0.56
171    1200           0.00  10719.08   55.64   54.66   56.65    0.56
200    1400           0.00   8007.08   55.46   54.32   56.65    0.55
228    1600           0.00   5947.90   55.54   54.68   56.44    0.56
257    1800           0.00   4524.45   55.62   54.05   57.30    0.56
285    2000           0.00   3397.08   56.52   54.60   58.58    0.57
314    2200           0.00   2733.63   56.01   54.58   57.51    0.56
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 21:35:06,083] [INFO] Set up nlp object from config
[2022-09-10 21:35:06,092] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 21:35:06,092] [INFO] Resuming training for: ['ner']
[2022-09-10 21:35:06,102] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 21:35:14,690] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 21:35:29,313] [INFO] Created vocabulary
[2022-09-10 21:35:30,828] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 21:35:32,524] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.77   70.64   67.57   74.00    0.71
 28     200           0.00  69991.04   71.71   74.49   69.13    0.72
 57     400           0.00  48978.51   69.78   73.54   66.38    0.70
 85     600           0.00  33515.72   69.35   73.63   65.54    0.69
114     800           0.00  24069.01   67.94   72.60   63.85    0.68
142    1000           0.00  17828.10   66.45   69.52   63.64    0.66
171    1200           0.00  12728.90   67.54   70.16   65.12    0.68
200    1400           0.00   9439.77   67.55   70.41   64.90    0.68
228    1600           0.00   7312.76   66.08   68.72   63.64    0.66
257    1800           0.00   5502.06   65.80   67.18   64.48    0.66
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 21:51:34,401] [INFO] Set up nlp object from config
[2022-09-10 21:51:34,411] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 21:51:34,411] [INFO] Resuming training for: ['ner']
[2022-09-10 21:51:34,420] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 21:51:43,066] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 21:51:57,809] [INFO] Created vocabulary
[2022-09-10 21:51:59,327] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 21:52:00,952] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    330.50   67.80   66.30   69.38    0.68
 28     200           0.00  63714.36   68.79   73.40   64.73    0.69
 57     400           0.00  43888.00   67.92   73.59   63.06    0.68
 85     600           0.00  28633.24   67.44   72.24   63.23    0.67
114     800           0.00  19565.40   66.96   72.03   62.56    0.67
142    1000           0.00  13124.84   66.67   71.35   62.56    0.67
171    1200           0.00   9090.30   65.59   70.85   61.06    0.66
200    1400           0.00   6284.91   65.78   70.61   61.56    0.66
228    1600           0.00   4534.22   65.00   69.92   60.73    0.65
257    1800           0.00   3672.15   63.86   69.26   59.23    0.64
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 22:09:36,232] [INFO] Set up nlp object from config
[2022-09-10 22:09:36,244] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 22:09:36,244] [INFO] Resuming training for: ['ner']
[2022-09-10 22:09:36,254] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 22:09:44,841] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 22:09:59,596] [INFO] Created vocabulary
[2022-09-10 22:10:01,098] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 22:10:02,738] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.80   66.92   65.12   68.82    0.67
 28     200           0.00  52233.94   66.40   68.93   64.06    0.66
 57     400           0.00  36313.90   65.80   69.64   62.37    0.66
 85     600           0.00  22878.57   65.15   69.32   61.44    0.65
114     800           0.00  15448.49   64.57   67.85   61.60    0.65
142    1000           0.00  10466.06   64.96   67.61   62.52    0.65
171    1200           0.00   7162.10   64.31   67.45   61.44    0.64
200    1400           0.00   5344.12   64.37   67.39   61.60    0.64
228    1600           0.00   3977.74   63.33   66.78   60.22    0.63
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 22:25:27,356] [INFO] Set up nlp object from config
[2022-09-10 22:25:27,367] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 22:25:27,367] [INFO] Resuming training for: ['ner']
[2022-09-10 22:25:27,376] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 22:25:35,805] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 22:25:50,093] [INFO] Created vocabulary
[2022-09-10 22:25:51,562] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 22:25:53,253] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.47   65.36   63.29   67.57    0.65
 28     200           0.00  60299.26   63.40   65.70   61.26    0.63
 57     400           0.00  41536.40   64.91   69.31   61.04    0.65
 85     600           0.00  27874.94   65.71   70.26   61.71    0.66
114     800           0.00  19593.30   65.47   69.72   61.71    0.65
142    1000           0.00  14080.85   65.32   69.10   61.94    0.65
171    1200           0.00   9987.77   65.07   69.39   61.26    0.65
200    1400           0.00   7207.08   64.43   68.80   60.59    0.64
228    1600           0.00   5361.14   64.21   68.01   60.81    0.64
257    1800           0.00   4115.17   63.79   68.21   59.91    0.64
285    2000           0.00   3235.68   63.46   68.04   59.46    0.63
314    2200           0.00   2600.08   63.06   67.70   59.01    0.63
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
Total DS: 133, Train: 52 Val: 14 Test: 67
Experiment: 1

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.59      0.59      0.59       213

   micro avg       0.59      0.59      0.59       213
   macro avg       0.59      0.59      0.59       213
weighted avg       0.59      0.59      0.59       213

Precision given by SeqEval: 59.15%
Recall given by SeqEval: 59.15%
F1-Score given by SeqEval: 59.15%
Accuracy given by SeqEval: 98.73%
------------------------------------------------------------
Experiment: 2

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.53      0.51      0.52       131

   micro avg       0.53      0.51      0.52       131
   macro avg       0.53      0.51      0.52       131
weighted avg       0.53      0.51      0.52       131

Precision given by SeqEval: 53.17%
Recall given by SeqEval: 51.15%
F1-Score given by SeqEval: 52.14%
Accuracy given by SeqEval: 99.09%
------------------------------------------------------------
Experiment: 3

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.61      0.38      0.47       316

   micro avg       0.61      0.38      0.47       316
   macro avg       0.61      0.38      0.47       316
weighted avg       0.61      0.38      0.47       316

Precision given by SeqEval: 61.34%
Recall given by SeqEval: 37.66%
F1-Score given by SeqEval: 46.67%
Accuracy given by SeqEval: 98.41%
------------------------------------------------------------
Experiment: 4

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.54      0.47      0.50       124

   micro avg       0.54      0.47      0.50       124
   macro avg       0.54      0.47      0.50       124
weighted avg       0.54      0.47      0.50       124

Precision given by SeqEval: 53.70%
Recall given by SeqEval: 46.77%
F1-Score given by SeqEval: 50.00%
Accuracy given by SeqEval: 98.78%
------------------------------------------------------------
Experiment: 5

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.79      0.73      0.76       132

   micro avg       0.79      0.73      0.76       132
   macro avg       0.79      0.73      0.76       132
weighted avg       0.79      0.73      0.76       132

Precision given by SeqEval: 79.34%
Recall given by SeqEval: 72.73%
F1-Score given by SeqEval: 75.89%
Accuracy given by SeqEval: 98.64%
------------------------------------------------------------
Experiment: 6

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.48      0.48      0.48       201

   micro avg       0.48      0.48      0.48       201
   macro avg       0.48      0.48      0.48       201
weighted avg       0.48      0.48      0.48       201

Precision given by SeqEval: 47.78%
Recall given by SeqEval: 48.26%
F1-Score given by SeqEval: 48.02%
Accuracy given by SeqEval: 97.98%
------------------------------------------------------------
Experiment: 7

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.59      0.69      0.64       146

   micro avg       0.59      0.69      0.64       146
   macro avg       0.59      0.69      0.64       146
weighted avg       0.59      0.69      0.64       146

Precision given by SeqEval: 58.72%
Recall given by SeqEval: 69.18%
F1-Score given by SeqEval: 63.52%
Accuracy given by SeqEval: 98.34%
------------------------------------------------------------
Experiment: 8

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.60      0.58      0.59       202

   micro avg       0.60      0.58      0.59       202
   macro avg       0.60      0.58      0.59       202
weighted avg       0.60      0.58      0.59       202

Precision given by SeqEval: 59.60%
Recall given by SeqEval: 58.42%
F1-Score given by SeqEval: 59.00%
Accuracy given by SeqEval: 98.49%
------------------------------------------------------------
Experiment: 9

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.69      0.67      0.68       163

   micro avg       0.69      0.67      0.68       163
   macro avg       0.69      0.67      0.68       163
weighted avg       0.69      0.67      0.68       163

Precision given by SeqEval: 69.18%
Recall given by SeqEval: 67.48%
F1-Score given by SeqEval: 68.32%
Accuracy given by SeqEval: 98.64%
------------------------------------------------------------
Experiment: 10

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.63      0.64      0.64       219

   micro avg       0.63      0.64      0.64       219
   macro avg       0.63      0.64      0.64       219
weighted avg       0.63      0.64      0.64       219

Precision given by SeqEval: 63.23%
Recall given by SeqEval: 64.38%
F1-Score given by SeqEval: 63.80%
Accuracy given by SeqEval: 98.99%
------------------------------------------------------------
Experiment: 11

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.73      0.77      0.75       106

   micro avg       0.73      0.77      0.75       106
   macro avg       0.73      0.77      0.75       106
weighted avg       0.73      0.77      0.75       106

Precision given by SeqEval: 73.21%
Recall given by SeqEval: 77.36%
F1-Score given by SeqEval: 75.23%
Accuracy given by SeqEval: 99.67%
------------------------------------------------------------
Experiment: 12

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.65      0.64      0.64       230

   micro avg       0.65      0.64      0.64       230
   macro avg       0.65      0.64      0.64       230
weighted avg       0.65      0.64      0.64       230

Precision given by SeqEval: 64.63%
Recall given by SeqEval: 64.35%
F1-Score given by SeqEval: 64.49%
Accuracy given by SeqEval: 96.78%
------------------------------------------------------------
Experiment: 13

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.73      0.48      0.58       314

   micro avg       0.73      0.48      0.58       314
   macro avg       0.73      0.48      0.58       314
weighted avg       0.73      0.48      0.58       314

Precision given by SeqEval: 72.60%
Recall given by SeqEval: 48.09%
F1-Score given by SeqEval: 57.85%
Accuracy given by SeqEval: 98.46%
------------------------------------------------------------
Experiment: 14

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.69      0.77      0.73       252

   micro avg       0.69      0.77      0.73       252
   macro avg       0.69      0.77      0.73       252
weighted avg       0.69      0.77      0.73       252

Precision given by SeqEval: 69.29%
Recall given by SeqEval: 76.98%
F1-Score given by SeqEval: 72.93%[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 22:44:56,149] [INFO] Set up nlp object from config
[2022-09-10 22:44:56,158] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 22:44:56,158] [INFO] Resuming training for: ['ner']
[2022-09-10 22:44:56,167] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 22:45:04,597] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 22:45:19,090] [INFO] Created vocabulary
[2022-09-10 22:45:20,568] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 22:45:22,213] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    112.83   61.74   59.14   64.57    0.62
 28     200           0.00  60032.01   62.57   64.86   60.43    0.63
 57     400           0.00  42351.97   63.00   66.67   59.71    0.63
 85     600           0.00  27979.97   61.27   65.98   57.19    0.61
114     800           0.00  19230.78   61.79   65.52   58.45    0.62
142    1000           0.00  13689.96   61.55   65.00   58.45    0.62
171    1200           0.00   9400.12   59.15   63.69   55.22    0.59
200    1400           0.00   6854.10   59.07   63.75   55.04    0.59
228    1600           0.00   5225.22   59.17   63.51   55.40    0.59
257    1800           0.00   3989.42   58.85   63.22   55.04    0.59
285    2000           0.00   3190.95   59.54   64.11   55.58    0.60
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 23:02:19,163] [INFO] Set up nlp object from config
[2022-09-10 23:02:19,172] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 23:02:19,172] [INFO] Resuming training for: ['ner']
[2022-09-10 23:02:19,181] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 23:02:27,590] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 23:02:42,049] [INFO] Created vocabulary
[2022-09-10 23:02:43,531] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 23:02:45,215] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.00   67.18   61.94   73.40    0.67
 28     200           0.00  57612.02   69.20   66.38   72.26    0.69
 57     400           0.00  40034.99   70.34   68.52   72.26    0.70
 85     600           0.00  26680.90   71.34   69.73   73.02    0.71
114     800           0.00  18233.12   70.40   68.64   72.26    0.70
142    1000           0.00  13111.25   69.33   66.78   72.08    0.69
171    1200           0.00   9309.19   69.36   67.50   71.32    0.69
200    1400           0.00   6922.75   68.80   66.79   70.94    0.69
228    1600           0.00   5336.45   68.88   67.27   70.57    0.69
257    1800           0.00   4054.41   67.45   64.92   70.19    0.67
285    2000           0.00   3332.29   67.76   65.49   70.19    0.68
314    2200           0.00   2694.51   67.70   65.72   69.81    0.68
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 23:20:58,852] [INFO] Set up nlp object from config
[2022-09-10 23:20:58,862] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 23:20:58,862] [INFO] Resuming training for: ['ner']
[2022-09-10 23:20:58,871] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 23:21:07,514] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 23:21:22,313] [INFO] Created vocabulary
[2022-09-10 23:21:23,830] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 23:21:25,443] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    188.87   48.99   47.13   51.00    0.49
 33     200           0.00  69126.37   47.74   50.00   45.68    0.48
 66     400           0.00  45822.13   47.98   51.66   44.79    0.48
100     600           0.00  29204.52   48.46   51.90   45.45    0.48
133     800           0.00  19716.50   48.46   51.90   45.45    0.48
166    1000           0.00  13432.45   49.36   51.70   47.23    0.49
200    1200           0.00   9277.82   49.31   51.32   47.45    0.49
233    1400           0.00   6397.61   49.30   52.37   46.56    0.49
266    1600           0.00   4671.85   50.00   52.84   47.45    0.50
300    1800           0.00   3560.62   50.35   52.80   48.12    0.50
333    2000           0.00   2883.21   49.88   52.58   47.45    0.50
366    2200           0.00   2255.62   49.59   52.48   47.01    0.50
400    2400           0.00   1839.29   49.24   51.97   46.78    0.49
433    2600           0.00   1548.20   49.47   52.22   47.01    0.49
466    2800           0.00   1364.74   49.59   52.48   47.01    0.50
500    3000           0.00   1206.04   49.23   52.79   46.12    0.49
533    3200           0.00   1012.53   48.95   51.86   46.34    0.49
566    3400           0.00    995.11   49.65   53.16   46.56    0.50
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-10 23:52:49,919] [INFO] Set up nlp object from config
[2022-09-10 23:52:49,928] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-10 23:52:49,928] [INFO] Resuming training for: ['ner']
[2022-09-10 23:52:49,937] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-10 23:52:58,418] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-10 23:53:12,843] [INFO] Created vocabulary
[2022-09-10 23:53:14,314] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-10 23:53:15,983] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.07   65.85   64.53   67.22    0.66
 28     200           0.00  71604.22   66.30   69.91   63.05    0.66
 57     400           0.00  51403.12   66.74   70.11   63.67    0.67
 85     600           0.00  35123.46   67.47   71.23   64.09    0.67
114     800           0.00  25051.62   67.10   69.18   65.14    0.67
142    1000           0.00  18340.20   66.81   68.11   65.55    0.67
171    1200           0.00  13305.60   66.03   67.17   64.93    0.66
200    1400           0.00   9779.11   65.18   66.31   64.09    0.65
228    1600           0.00   7315.49   64.18   65.58   62.84    0.64
257    1800           0.00   5589.05   63.93   64.41   63.47    0.64
285    2000           0.00   4400.15   63.21   64.03   62.42    0.63
314    2200           0.00   3590.33   63.45   63.85   63.05    0.63
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 00:11:58,819] [INFO] Set up nlp object from config
[2022-09-11 00:11:58,829] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 00:11:58,829] [INFO] Resuming training for: ['ner']
[2022-09-11 00:11:58,839] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 00:12:07,290] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 00:12:21,653] [INFO] Created vocabulary
[2022-09-11 00:12:23,135] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 00:12:24,810] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    371.51   61.16   61.52   60.79    0.61
 28     200           0.00  59513.02   62.77   67.82   58.42    0.63
 57     400           0.00  41296.28   64.14   70.81   58.61    0.64
 85     600           0.00  26964.17   62.74   68.30   58.02    0.63
114     800           0.00  18494.25   62.23   67.92   57.43    0.62
142    1000           0.00  12836.80   60.24   65.65   55.64    0.60
171    1200           0.00   8845.69   60.28   65.21   56.04    0.60
200    1400           0.00   6402.66   60.13   65.13   55.84    0.60
228    1600           0.00   4550.45   59.45   63.31   56.04    0.59
257    1800           0.00   3499.47   59.00   62.53   55.84    0.59
285    2000           0.00   2698.94   58.53   62.47   55.05    0.59
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 00:30:47,190] [INFO] Set up nlp object from config
[2022-09-11 00:30:47,200] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 00:30:47,200] [INFO] Resuming training for: ['ner']
[2022-09-11 00:30:47,209] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 00:30:55,718] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 00:31:10,265] [INFO] Created vocabulary
[2022-09-11 00:31:11,749] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 00:31:13,377] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.52   63.31   60.27   66.67    0.63
 28     200           0.00  59220.01   64.00   65.20   62.85    0.64
 57     400           0.00  42610.40   66.08   68.65   63.69    0.66
 85     600           0.00  28388.80   65.28   67.19   63.48    0.65
114     800           0.00  19835.74   65.36   67.11   63.69    0.65
142    1000           0.00  14238.83   65.01   66.15   63.91    0.65
171    1200           0.00   9920.49   64.33   66.37   62.42    0.64
200    1400           0.00   7073.78   64.91   67.12   62.85    0.65
228    1600           0.00   5180.57   63.96   66.06   62.00    0.64
257    1800           0.00   3878.77   63.89   66.14   61.78    0.64
285    2000           0.00   3059.10   64.43   65.85   63.06    0.64
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 00:48:53,151] [INFO] Set up nlp object from config
[2022-09-11 00:48:53,162] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 00:48:53,162] [INFO] Resuming training for: ['ner']
[2022-09-11 00:48:53,172] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 00:49:01,709] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 00:49:16,476] [INFO] Created vocabulary
[2022-09-11 00:49:17,978] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 00:49:19,641] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.52   60.78   58.74   62.96    0.61
 28     200           0.00  64123.08   61.59   64.37   59.04    0.62
 57     400           0.00  46029.70   61.34   66.58   56.86    0.61
 85     600           0.00  31787.61   62.41   66.75   58.61    0.62
114     800           0.00  22631.52   62.72   67.16   58.82    0.63
142    1000           0.00  16861.07   63.15   66.75   59.91    0.63
171    1200           0.00  12475.42   62.69   67.68   58.39    0.63
200    1400           0.00   9348.68   61.40   65.84   57.52    0.61
228    1600           0.00   7063.60   60.26   65.23   55.99    0.60
257    1800           0.00   5543.58   61.49   64.66   58.61    0.61
285    2000           0.00   4536.10   62.05   65.93   58.61    0.62
314    2200           0.00   3514.14   61.04   65.02   57.52    0.61
342    2400           0.00   3078.92   60.28   64.13   56.86    0.60
371    2600           0.00   2587.28   60.90   64.71   57.52    0.61
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 01:15:13,635] [INFO] Set up nlp object from config
[2022-09-11 01:15:13,646] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 01:15:13,647] [INFO] Resuming training for: ['ner']
[2022-09-11 01:15:13,656] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 01:15:22,209] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 01:15:36,820] [INFO] Created vocabulary
[2022-09-11 01:15:38,323] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 01:15:39,984] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    114.68   55.18   54.79   55.58    0.55
 28     200           0.00  61888.15   54.72   57.67   52.07    0.55
 57     400           0.00  43920.37   55.38   60.54   51.03    0.55
 85     600           0.00  29085.29   55.58   60.44   51.45    0.56
114     800           0.00  20639.37   53.74   57.55   50.41    0.54
142    1000           0.00  14938.83   53.49   56.71   50.62    0.53
171    1200           0.00  10704.49   53.62   56.24   51.24    0.54
200    1400           0.00   7956.51   53.74   56.49   51.24    0.54
228    1600           0.00   5989.87   54.45   57.31   51.86    0.54
257    1800           0.00   4611.30   53.91   56.88   51.24    0.54
285    2000           0.00   3522.45   54.15   57.41   51.24    0.54
314    2200           0.00   2888.62   53.01   56.41   50.00    0.53
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 01:35:14,044] [INFO] Set up nlp object from config
[2022-09-11 01:35:14,054] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 01:35:14,054] [INFO] Resuming training for: ['ner']
[2022-09-11 01:35:14,063] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 01:35:22,489] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 01:35:36,883] [INFO] Created vocabulary
[2022-09-11 01:35:38,365] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 01:35:40,031] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    100.29   65.03   62.73   67.51    0.65
 28     200           0.00  66457.83   65.44   68.52   62.62    0.65
 57     400           0.00  46329.35   66.95   72.44   62.23    0.67
 85     600           0.00  30818.87   67.28   70.69   64.19    0.67
114     800           0.00  21169.61   66.11   70.26   62.43    0.66
142    1000           0.00  15105.12   65.71   68.88   62.82    0.66
171    1200           0.00  10837.09   64.70   67.37   62.23    0.65
200    1400           0.00   7915.71   65.31   67.79   63.01    0.65
228    1600           0.00   5938.52   64.45   66.39   62.62    0.64
257    1800           0.00   4571.46   64.63   67.23   62.23    0.65
285    2000           0.00   3769.73   64.44   66.81   62.23    0.64
314    2200           0.00   2911.98   64.09   66.74   61.64    0.64
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 01:53:15,472] [INFO] Set up nlp object from config
[2022-09-11 01:53:15,482] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 01:53:15,482] [INFO] Resuming training for: ['ner']
[2022-09-11 01:53:15,491] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 01:53:24,097] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 01:53:38,841] [INFO] Created vocabulary
[2022-09-11 01:53:40,357] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 01:53:41,994] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    117.50   59.70   55.51   64.57    0.60
 28     200           0.00  59615.83   60.64   59.38   61.96    0.61
 57     400           0.00  42303.23   62.08   62.42   61.74    0.62
 85     600           0.00  27610.87   61.32   62.00   60.65    0.61
114     800           0.00  18610.46   60.18   60.57   59.78    0.60
142    1000           0.00  13070.11   60.66   59.83   61.52    0.61
171    1200           0.00   9078.88   60.66   61.33   60.00    0.61
200    1400           0.00   6605.75   60.63   60.83   60.43    0.61
228    1600           0.00   4987.85   59.93   59.87   60.00    0.60
257    1800           0.00   3798.63   60.43   60.43   60.43    0.60
285    2000           0.00   3051.39   59.96   60.35   59.57    0.60
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 02:09:52,340] [INFO] Set up nlp object from config
[2022-09-11 02:09:52,349] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 02:09:52,349] [INFO] Resuming training for: ['ner']
[2022-09-11 02:09:52,358] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 02:10:00,826] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 02:10:15,293] [INFO] Created vocabulary
[2022-09-11 02:10:16,764] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 02:10:18,420] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.52   65.79   64.87   66.74    0.66
 28     200           0.00  64526.97   65.87   69.48   62.63    0.66
 57     400           0.00  45646.90   65.81   68.36   63.45    0.66
 85     600           0.00  30967.01   65.40   67.47   63.45    0.65
114     800           0.00  22045.54   65.13   66.45   63.86    0.65
142    1000           0.00  16146.17   64.51   65.61   63.45    0.65
171    1200           0.00  11904.47   64.46   64.86   64.07    0.64
200    1400           0.00   8943.15   64.38   65.13   63.66    0.64
228    1600           0.00   6638.01   63.76   64.50   63.04    0.64
257    1800           0.00   5163.69   64.72   65.61   63.86    0.65
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 02:26:29,309] [INFO] Set up nlp object from config
[2022-09-11 02:26:29,318] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 02:26:29,319] [INFO] Resuming training for: ['ner']
[2022-09-11 02:26:29,328] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 02:26:37,900] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 02:26:52,615] [INFO] Created vocabulary
[2022-09-11 02:26:54,120] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 02:26:55,760] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.01   62.86   61.09   64.75    0.63
 28     200           0.00  59879.43   63.88   67.07   60.98    0.64
 57     400           0.00  42468.24   65.13   67.70   62.75    0.65
 85     600           0.00  28024.89   66.90   69.71   64.30    0.67
114     800           0.00  19508.46   66.13   68.32   64.08    0.66
142    1000           0.00  14069.60   66.06   68.41   63.86    0.66
171    1200           0.00  10093.70   65.37   67.70   63.19    0.65
200    1400           0.00   7414.81   65.28   68.03   62.75    0.65
228    1600           0.00   5526.66   64.43   67.23   61.86    0.64
257    1800           0.00   4105.14   63.85   65.73   62.08    0.64
285    2000           0.00   3420.80   63.31   65.33   61.42    0.63
314    2200           0.00   2774.27   63.34   64.88   61.86    0.63
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 02:45:38,698] [INFO] Set up nlp object from config
[2022-09-11 02:45:38,708] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 02:45:38,708] [INFO] Resuming training for: ['ner']
[2022-09-11 02:45:38,717] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 02:45:47,234] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 02:46:01,721] [INFO] Created vocabulary
[2022-09-11 02:46:03,195] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 02:46:04,809] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.80   70.44   66.53   74.83    0.70
 28     200           0.00  56084.56   71.52   70.90   72.16    0.72
 57     400           0.00  40768.53   72.55   71.83   73.27    0.73
 85     600           0.00  27711.33   73.27   72.39   74.16    0.73
114     800           0.00  19409.19   72.51   71.34   73.72    0.73
142    1000           0.00  13988.32   71.93   70.84   73.05    0.72
171    1200           0.00  10198.38   70.72   68.92   72.61    0.71
200    1400           0.00   7716.64   70.26   68.06   72.61    0.70
228    1600           0.00   5657.46   69.35   67.73   71.05    0.69
257    1800           0.00   4471.64   69.27   67.58   71.05    0.69
285    2000           0.00   3677.57   68.99   67.45   70.60    0.69
314    2200           0.00   2861.61   68.62   66.95   70.38    0.69
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 03:05:34,508] [INFO] Set up nlp object from config
[2022-09-11 03:05:34,518] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 03:05:34,518] [INFO] Resuming training for: ['ner']
[2022-09-11 03:05:34,527] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 03:05:43,089] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 03:05:57,862] [INFO] Created vocabulary
[2022-09-11 03:05:59,389] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 03:06:01,067] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    136.24   65.10   58.91   72.73    0.65
 28     200           0.00  55507.77   66.67   63.56   70.10    0.67
 57     400           0.00  38055.07   67.50   64.49   70.81    0.68
 85     600           0.00  24630.89   65.74   63.68   67.94    0.66
114     800           0.00  16442.29   65.53   62.66   68.66    0.66
142    1000           0.00  11444.32   66.44   63.54   69.62    0.66
171    1200           0.00   7903.95   65.37   62.58   68.42    0.65
200    1400           0.00   5789.59   65.98   62.91   69.38    0.66
228    1600           0.00   4340.74   65.13   62.75   67.70    0.65
257    1800           0.00   3200.96   65.13   62.75   67.70    0.65
285    2000           0.00   2575.21   64.58   62.75   66.51    0.65
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last

Accuracy given by SeqEval: 98.28%
------------------------------------------------------------
Experiment: 15

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.69      0.62      0.65       203

   micro avg       0.69      0.62      0.65       203
   macro avg       0.69      0.62      0.65       203
weighted avg       0.69      0.62      0.65       203

Precision given by SeqEval: 69.23%
Recall given by SeqEval: 62.07%
F1-Score given by SeqEval: 65.45%
Accuracy given by SeqEval: 98.57%
------------------------------------------------------------
Experiment: 16

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.61      0.60      0.60       191

   micro avg       0.61      0.60      0.60       191
   macro avg       0.61      0.60      0.60       191
weighted avg       0.61      0.60      0.60       191

Precision given by SeqEval: 61.29%
Recall given by SeqEval: 59.69%
F1-Score given by SeqEval: 60.48%
Accuracy given by SeqEval: 98.34%
------------------------------------------------------------
Experiment: 17

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.56      0.61      0.59       114

   micro avg       0.56      0.61      0.59       114
   macro avg       0.56      0.61      0.59       114
weighted avg       0.56      0.61      0.59       114

Precision given by SeqEval: 56.00%
Recall given by SeqEval: 61.40%
F1-Score given by SeqEval: 58.58%
Accuracy given by SeqEval: 99.14%
------------------------------------------------------------
Experiment: 18

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.64      0.56      0.60       356

   micro avg       0.64      0.56      0.60       356
   macro avg       0.64      0.56      0.60       356
weighted avg       0.64      0.56      0.60       356

Precision given by SeqEval: 64.10%
Recall given by SeqEval: 56.18%
F1-Score given by SeqEval: 59.88%
Accuracy given by SeqEval: 98.66%
------------------------------------------------------------
Experiment: 19

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.67      0.74      0.70       489

   micro avg       0.67      0.74      0.70       489
   macro avg       0.67      0.74      0.70       489
weighted avg       0.67      0.74      0.70       489

Precision given by SeqEval: 67.23%
Recall given by SeqEval: 73.82%
F1-Score given by SeqEval: 70.37%
Accuracy given by SeqEval: 98.99%
------------------------------------------------------------
Experiment: 20

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.54      0.64      0.58       228

   micro avg       0.54      0.64      0.58       228
   macro avg       0.54      0.64      0.58       228
weighted avg       0.54      0.64      0.58       228

Precision given by SeqEval: 53.70%
Recall given by SeqEval: 63.60%
F1-Score given by SeqEval: 58.23%
Accuracy given by SeqEval: 97.29%
------------------------------------------------------------
Experiment: 21

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.56      0.61      0.58       140

   micro avg       0.56      0.61      0.58       140
   macro avg       0.56      0.61      0.58       140
weighted avg       0.56      0.61      0.58       140

Precision given by SeqEval: 56.29%
Recall given by SeqEval: 60.71%
F1-Score given by SeqEval: 58.42%
Accuracy given by SeqEval: 99.03%
------------------------------------------------------------
Experiment: 22

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.65      0.61      0.63       119

   micro avg       0.65      0.61      0.63       119
   macro avg       0.65      0.61      0.63       119
weighted avg       0.65      0.61      0.63       119

Precision given by SeqEval: 65.18%
Recall given by SeqEval: 61.34%
F1-Score given by SeqEval: 63.20%
Accuracy given by SeqEval: 99.25%
------------------------------------------------------------
Experiment: 23

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.62      0.73      0.67       128

   micro avg       0.62      0.73      0.67       128
   macro avg       0.62      0.73      0.67       128
weighted avg       0.62      0.73      0.67       128

Precision given by SeqEval: 62.42%
Recall given by SeqEval: 72.66%
F1-Score given by SeqEval: 67.15%
Accuracy given by SeqEval: 99.30%
------------------------------------------------------------
Experiment: 24

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.50      0.43      0.46       244

   micro avg       0.50      0.43      0.46       244
   macro avg       0.50      0.43      0.46       244
weighted avg       0.50      0.43      0.46       244

Precision given by SeqEval: 50.00%
Recall given by SeqEval: 43.44%
F1-Score given by SeqEval: 46.49%
Accuracy given by SeqEval: 98.84%
------------------------------------------------------------
Experiment: 25

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.69      0.61      0.65       115

   micro avg       0.69      0.61      0.65       115
   macro avg       0.69      0.61      0.65       115
weighted avg       0.69      0.61      0.65       115

Precision given by SeqEval: 69.31%
Recall given by SeqEval: 60.87%
F1-Score given by SeqEval: 64.81%
Accuracy given by SeqEval: 99.20%
------------------------------------------------------------
Experiment: 26

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.73      0.64      0.68       365

   micro avg       0.73      0.64      0.68       365
   macro avg       0.73      0.64      0.68       365
weighted avg       0.73      0.64      0.68       365

Precision given by SeqEval: 72.76%
Recall given by SeqEval: 64.38%
F1-Score given by SeqEval: 68.31%
Accuracy given by SeqEval: 98.42%
------------------------------------------------------------
Experiment: 27

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.67      0.63      0.65       173

   micro avg       0.67      0.63      0.65       173
   macro avg       0.67      0.63      0.65       173
weighted avg       0.67      0.63      0.65       173

Precision given by SeqEval: 66.87%
Recall given by SeqEval: 63.01%
F1-Score given by SeqEval: 64.88%
Accuracy given by SeqEval: 99.20%
------------------------------------------------------------
Experiment: 28

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.71      0.74      0.72       214

   micro avg       0.71      0.74      0.72       214
   macro avg       0.71      0.74      0.72       214
weighted avg       0.71      0.74      0.72       214

Precision given by SeqEval: 70.85%[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 03:24:33,909] [INFO] Set up nlp object from config
[2022-09-11 03:24:33,919] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 03:24:33,919] [INFO] Resuming training for: ['ner']
[2022-09-11 03:24:33,928] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 03:24:42,374] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 03:24:56,848] [INFO] Created vocabulary
[2022-09-11 03:24:58,326] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 03:24:59,944] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.77   57.23   50.34   66.29    0.57
 28     200           0.00  54003.15   58.87   54.65   63.80    0.59
 57     400           0.00  38055.52   61.03   57.93   64.48    0.61
 85     600           0.00  25551.30   60.60   57.52   64.03    0.61
114     800           0.00  17554.56   60.15   57.26   63.35    0.60
142    1000           0.00  12624.98   59.89   56.80   63.35    0.60
171    1200           0.00   8791.94   60.52   57.55   63.80    0.61
200    1400           0.00   6582.92   58.60   55.20   62.44    0.59
228    1600           0.00   4952.93   58.67   55.69   61.99    0.59
257    1800           0.00   3718.02   58.46   55.49   61.76    0.58
285    2000           0.00   2963.38   58.90   56.29   61.76    0.59
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 03:41:59,548] [INFO] Set up nlp object from config
[2022-09-11 03:41:59,557] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 03:41:59,557] [INFO] Resuming training for: ['ner']
[2022-09-11 03:41:59,566] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 03:42:08,157] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 03:42:22,712] [INFO] Created vocabulary
[2022-09-11 03:42:24,220] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 03:42:25,880] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.47   52.71   48.22   58.11    0.53
 28     200           0.00  57400.10   51.03   51.86   50.23    0.51
 57     400           0.00  40105.66   51.29   53.41   49.32    0.51
 85     600           0.00  26446.29   52.88   55.28   50.68    0.53
114     800           0.00  18132.86   54.06   55.74   52.48    0.54
142    1000           0.00  12753.59   54.38   55.66   53.15    0.54
171    1200           0.00   8981.78   54.99   56.70   53.38    0.55
200    1400           0.00   6624.69   54.92   56.56   53.38    0.55
228    1600           0.00   5060.79   54.55   55.76   53.38    0.55
257    1800           0.00   3759.90   54.59   56.35   52.93    0.55
285    2000           0.00   3121.52   54.42   56.25   52.70    0.54
314    2200           0.00   2496.73   54.38   55.66   53.15    0.54
342    2400           0.00   2116.01   54.32   55.53   53.15    0.54
371    2600           0.00   1670.77   54.07   55.01   53.15    0.54
400    2800           0.00   1461.99   53.69   54.95   52.48    0.54
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 04:05:06,329] [INFO] Set up nlp object from config
[2022-09-11 04:05:06,339] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 04:05:06,339] [INFO] Resuming training for: ['ner']
[2022-09-11 04:05:06,351] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 04:05:14,932] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 04:05:29,488] [INFO] Created vocabulary
[2022-09-11 04:05:30,971] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 04:05:32,642] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    100.29   58.57   55.44   62.08    0.59
 28     200           0.00  55327.46   57.53   57.27   57.79    0.58
 57     400           0.00  38989.03   59.55   59.28   59.82    0.60
 85     600           0.00  25570.65   60.62   59.44   61.85    0.61
114     800           0.00  17632.07   60.36   59.56   61.17    0.60
142    1000           0.00  13088.03   60.07   58.58   61.63    0.60
171    1200           0.00   9410.49   60.89   59.96   61.85    0.61
200    1400           0.00   6982.79   60.00   59.08   60.95    0.60
228    1600           0.00   5364.63   59.07   57.92   60.27    0.59
257    1800           0.00   4071.49   60.20   58.84   61.63    0.60
285    2000           0.00   3267.19   59.63   58.15   61.17    0.60
314    2200           0.00   2655.07   59.42   58.39   60.50    0.59
342    2400           0.00   2240.47   59.38   58.10   60.72    0.59
371    2600           0.00   1897.19   59.41   57.94   60.95    0.59
400    2800           0.00   1584.46   58.54   57.52   59.59    0.59
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 04:28:34,290] [INFO] Set up nlp object from config
[2022-09-11 04:28:34,299] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 04:28:34,299] [INFO] Resuming training for: ['ner']
[2022-09-11 04:28:34,310] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 04:28:42,757] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 04:28:57,238] [INFO] Created vocabulary
[2022-09-11 04:28:58,707] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 04:29:00,305] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     98.75   49.12   45.12   53.88    0.49
 28     200           0.00  68774.87   51.83   51.83   51.83    0.52
 57     400           0.00  48425.80   52.14   52.94   51.37    0.52
 85     600           0.00  32749.52   53.15   54.29   52.05    0.53
114     800           0.00  23435.49   52.34   53.59   51.14    0.52
142    1000           0.00  17118.90   50.98   51.76   50.23    0.51
171    1200           0.00  12541.07   50.68   50.68   50.68    0.51
200    1400           0.00   9335.60   49.60   50.12   49.09    0.50
228    1600           0.00   6985.04   49.14   49.42   48.86    0.49
257    1800           0.00   5472.86   48.47   48.09   48.86    0.48
285    2000           0.00   4379.78   48.05   48.17   47.95    0.48
314    2200           0.00   3615.01   47.66   47.61   47.72    0.48
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 04:47:38,308] [INFO] Set up nlp object from config
[2022-09-11 04:47:38,318] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 04:47:38,318] [INFO] Resuming training for: ['ner']
[2022-09-11 04:47:38,327] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 04:47:46,781] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 04:48:01,232] [INFO] Created vocabulary
[2022-09-11 04:48:02,702] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 04:48:04,338] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    262.21   55.18   51.17   59.88    0.55
 28     200           0.00  52434.54   54.04   55.08   53.03    0.54
 57     400           0.00  37471.85   56.11   57.49   54.79    0.56
 85     600           0.00  25136.24   56.39   58.09   54.79    0.56
114     800           0.00  17266.96   56.44   56.72   56.16    0.56
142    1000           0.00  11994.72   55.67   55.19   56.16    0.56
171    1200           0.00   8362.59   54.34   53.51   55.19    0.54
200    1400           0.00   6220.71   54.46   54.51   54.40    0.54
228    1600           0.00   4582.79   54.16   53.54   54.79    0.54
257    1800           0.00   3624.54   53.49   52.98   54.01    0.53
285    2000           0.00   3004.82   53.45   53.09   53.82    0.53
314    2200           0.00   2302.76   52.34   53.24   51.47    0.52
342    2400           0.00   1979.29   53.52   53.41   53.62    0.54
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 05:07:15,887] [INFO] Set up nlp object from config
[2022-09-11 05:07:15,896] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 05:07:15,896] [INFO] Resuming training for: ['ner']
[2022-09-11 05:07:15,905] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 05:07:24,351] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 05:07:38,806] [INFO] Created vocabulary
[2022-09-11 05:07:40,276] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 05:07:41,916] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.01   59.61   56.06   63.65    0.60
 28     200           0.00  53391.17   60.50   61.93   59.14    0.61
 57     400           0.00  36465.11   60.65   62.68   58.74    0.61
 85     600           0.00  23248.93   61.14   62.20   60.12    0.61
114     800           0.00  15760.39   62.28   62.28   62.28    0.62
142    1000           0.00  10953.35   60.50   59.36   61.69    0.61
171    1200           0.00   7635.26   60.43   59.96   60.90    0.60
200    1400           0.00   5532.52   58.99   58.10   59.92    0.59
228    1600           0.00   4053.68   59.13   58.17   60.12    0.59
257    1800           0.00   3113.43   58.11   57.50   58.74    0.58
285    2000           0.00   2357.21   58.40   58.06   58.74    0.58
314    2200           0.00   1904.37   57.81   57.48   58.15    0.58
342    2400           0.00   1595.30   57.17   57.17   57.17    0.57
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 05:28:44,699] [INFO] Set up nlp object from config
[2022-09-11 05:28:44,708] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 05:28:44,709] [INFO] Resuming training for: ['ner']
[2022-09-11 05:28:44,718] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 05:28:53,376] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 05:29:08,026] [INFO] Created vocabulary
[2022-09-11 05:29:09,504] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 05:29:11,124] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.00   53.91   48.15   61.24    0.54
 28     200           0.00  60778.33   52.81   50.59   55.25    0.53
 57     400           0.00  42746.07   55.27   53.86   56.75    0.55
 85     600           0.00  28135.05   57.83   56.42   59.31    0.58
114     800           0.00  19044.01   58.21   56.57   59.96    0.58
142    1000           0.00  13339.48   58.21   56.57   59.96    0.58
171    1200           0.00   9531.24   58.48   56.88   60.17    0.58
200    1400           0.00   6971.94   58.43   57.17   59.74    0.58
228    1600           0.00   5335.94   57.88   55.75   60.17    0.58
257    1800           0.00   4042.30   57.62   55.82   59.53    0.58
285    2000           0.00   3324.26   57.92   56.79   59.10    0.58
314    2200           0.00   2698.46   57.23   55.67   58.89    0.57
342    2400           0.00   2315.91   57.66   56.88   58.46    0.58
371    2600           0.00   1904.29   57.08   55.58   58.67    0.57
400    2800           0.00   1634.60   56.54   55.33   57.82    0.57
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 05:53:03,050] [INFO] Set up nlp object from config
[2022-09-11 05:53:03,059] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 05:53:03,060] [INFO] Resuming training for: ['ner']
[2022-09-11 05:53:03,069] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 05:53:11,503] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 05:53:25,941] [INFO] Created vocabulary
[2022-09-11 05:53:27,411] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 05:53:29,047] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     98.75   60.80   60.23   61.37    0.61
 28     200           0.00  54141.57   59.28   64.97   54.50    0.59
 57     400           0.00  38529.04   61.26   67.04   56.40    0.61
 85     600           0.00  26007.70   61.25   66.03   57.11    0.61
114     800           0.00  18230.21   62.44   65.71   59.48    0.62
142    1000           0.00  13272.55   61.67   65.17   58.53    0.62
171    1200           0.00   9472.48   61.75   65.34   58.53    0.62
200    1400           0.00   7399.58   61.98   65.87   58.53    0.62
228    1600           0.00   5563.36   61.58   65.25   58.29    0.62
257    1800           0.00   4503.30   61.69   64.92   58.77    0.62
285    2000           0.00   3782.80   61.13   65.15   57.58    0.61
314    2200           0.00   3076.92   60.72   63.64   58.06    0.61
342    2400           0.00   2730.30   60.38   64.34   56.87    0.60
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 06:13:33,149] [INFO] Set up nlp object from config
[2022-09-11 06:13:33,159] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 06:13:33,159] [INFO] Resuming training for: ['ner']
[2022-09-11 06:13:33,169] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 06:13:41,772] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 06:13:56,549] [INFO] Created vocabulary
[2022-09-11 06:13:58,060] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 06:13:59,682] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    288.30   55.43   51.34   60.22    0.55
 33     200           0.00  68900.80   55.66   56.04   55.28    0.56
 66     400           0.00  47243.72   58.24   58.50   57.98    0.58
100     600           0.00  29651.17   58.84   58.57   59.10    0.59
133     800           0.00  19389.27   57.69   57.62   57.75    0.58
166    1000           0.00  13114.99   57.17   57.05   57.30    0.57
200    1200           0.00   8823.05   56.92   57.44   56.40    0.57
233    1400           0.00   6360.35   57.47   58.33   56.63    0.57
266    1600           0.00   4567.67   56.25   58.00   54.61    0.56
300    1800           0.00   3429.97   56.98   58.04   55.96    0.57
333    2000           0.00   2681.94   56.68   58.16   55.28    0.57
366    2200           0.00   2170.83   56.32   58.13   54.61    0.56
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 06:34:34,114] [INFO] Set up nlp object from config
[2022-09-11 06:34:34,124] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 06:34:34,124] [INFO] Resuming training for: ['ner']
[2022-09-11 06:34:34,133] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 06:34:42,547] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 06:34:57,035] [INFO] Created vocabulary
[2022-09-11 06:34:58,507] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 06:35:00,168] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.80   74.45   74.67   74.24    0.74
 28     200           0.00  61724.19   75.61   81.58   70.45    0.76
 57     400           0.00  41616.82   75.03   81.24   69.70    0.75
 85     600           0.00  27119.70   74.77   79.66   70.45    0.75
114     800           0.00  18868.25   74.47   79.70   69.89    0.74
142    1000           0.00  13388.12   74.10   78.85   69.89    0.74
171    1200           0.00   9406.05   74.28   78.08   70.83    0.74
200    1400           0.00   6899.95   73.95   78.51   69.89    0.74
228    1600           0.00   5164.18   73.26   78.40   68.75    0.73
257    1800           0.00   4030.05   72.91   77.85   68.56    0.73
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 06:52:17,012] [INFO] Set up nlp object from config
[2022-09-11 06:52:17,022] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 06:52:17,022] [INFO] Resuming training for: ['ner']
[2022-09-11 06:52:17,031] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 06:52:25,663] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 06:52:40,362] [INFO] Created vocabulary
[2022-09-11 06:52:41,865] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 06:52:43,510] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.01   62.81   61.81   63.83    0.63
 28     200           0.00  58336.02   63.72   66.43   61.22    0.64
 57     400           0.00  41566.35   64.37   68.13   61.00    0.64
 85     600           0.00  28310.35   64.06   67.97   60.57    0.64
114     800           0.00  19837.13   63.63   66.75   60.78    0.64
142    1000           0.00  14299.44   63.18   66.03   60.57    0.63
171    1200           0.00  10329.55   62.92   64.97   61.00    0.63
200    1400           0.00   7587.57   62.63   64.58   60.78    0.63
228    1600           0.00   5905.14   62.78   64.67   61.00    0.63
257    1800           0.00   4473.89   61.68   64.30   59.26    0.62
285    2000           0.00   3546.97   61.26   64.42   58.39    0.61
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 07:10:33,578] [INFO] Set up nlp object from config
[2022-09-11 07:10:33,588] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 07:10:33,588] [INFO] Resuming training for: ['ner']
[2022-09-11 07:10:33,597] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 07:10:42,187] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 07:10:56,889] [INFO] Created vocabulary
[2022-09-11 07:10:58,393] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 07:11:00,009] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    114.68   56.58   51.08   63.41    0.57
 28     200           0.00  62811.96   56.01   55.21   56.83    0.56
 57     400           0.00  43043.34   58.11   57.69   58.54    0.58
 85     600           0.00  28888.34   58.52   58.03   59.02    0.59
114     800           0.00  20313.55   57.18   56.10   58.29    0.57
142    1000           0.00  14427.45   57.45   56.64   58.29    0.57
171    1200           0.00  10389.92   57.68   55.96   59.51    0.58
200    1400           0.00   7695.26   57.90   57.28   58.54    0.58
228    1600           0.00   5724.17   56.83   56.35   57.32    0.57
257    1800           0.00   4429.46   57.07   56.59   57.56    0.57
285    2000           0.00   3538.10   56.45   55.85   57.07    0.56
314    2200           0.00   2878.02   56.62   56.42   56.83    0.57
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 07:29:44,355] [INFO] Set up nlp object from config
[2022-09-11 07:29:44,365] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 07:29:44,365] [INFO] Resuming training for: ['ner']
[2022-09-11 07:29:44,374] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 07:29:52,960] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 07:30:07,732] [INFO] Created vocabulary
[2022-09-11 07:30:09,259] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 07:30:10,872] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    110.45   65.17   67.12   63.34    0.65
 28     200           0.00  60626.75   65.80   73.27   59.71    0.66
 57     400           0.00  41375.76   65.05   73.62   58.26    0.65
 85     600           0.00  26725.60   64.65   72.62   58.26    0.65
114     800           0.00  18245.16   64.46   71.88   58.44    0.64
142    1000           0.00  12599.94   64.14   71.08   58.44    0.64
171    1200           0.00   8840.53   63.46   70.76   57.53    0.63
200    1400           0.00   6228.77   62.79   70.18   56.81    0.63
228    1600           0.00   4586.40   63.41   70.35   57.71    0.63
257    1800           0.00   3414.89   62.74   69.78   56.99    0.63
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 07:44:43,918] [INFO] Set up nlp object from config
[2022-09-11 07:44:43,927] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 07:44:43,927] [INFO] Resuming training for: ['ner']
[2022-09-11 07:44:43,937] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 07:44:52,522] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 07:45:07,146] [INFO] Created vocabulary
[2022-09-11 07:45:08,660] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 07:45:10,318] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.47   71.65   68.09   75.59    0.72
 28     200           0.00  64207.00   71.97   72.59   71.36    0.72
 57     400           0.00  45613.11   71.73   73.92   69.66    0.72
 85     600           0.00  31155.39   72.51   74.25   70.85    0.73
114     800           0.00  22291.56   71.37   73.35   69.49    0.71
142    1000           0.00  16258.26   71.00   71.68   70.34    0.71
171    1200           0.00  11512.61   71.01   71.88   70.17    0.71
200    1400           0.00   8349.73   69.94   71.10   68.81    0.70
228    1600           0.00   6199.68   68.92   70.44   67.46    0.69
257    1800           0.00   4719.39   68.69   70.34   67.12    0.69
285    2000           0.00   3662.14   68.39   69.72   67.12    0.68
314    2200           0.00   2882.27   67.60   70.13   65.25    0.68
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last

Recall given by SeqEval: 73.83%
F1-Score given by SeqEval: 72.31%
Accuracy given by SeqEval: 98.77%
------------------------------------------------------------
Experiment: 29

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.70      0.70      0.70       197

   micro avg       0.70      0.70      0.70       197
   macro avg       0.70      0.70      0.70       197
weighted avg       0.70      0.70      0.70       197

Precision given by SeqEval: 70.26%
Recall given by SeqEval: 69.54%
F1-Score given by SeqEval: 69.90%
Accuracy given by SeqEval: 98.54%
------------------------------------------------------------
Experiment: 30

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.64      0.74      0.69       124

   micro avg       0.64      0.74      0.69       124
   macro avg       0.64      0.74      0.69       124
weighted avg       0.64      0.74      0.69       124

Precision given by SeqEval: 64.34%
Recall given by SeqEval: 74.19%
F1-Score given by SeqEval: 68.91%
Accuracy given by SeqEval: 98.94%
------------------------------------------------------------
Experiment: 31

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.70      0.51      0.59       415

   micro avg       0.70      0.51      0.59       415
   macro avg       0.70      0.51      0.59       415
weighted avg       0.70      0.51      0.59       415

Precision given by SeqEval: 69.74%
Recall given by SeqEval: 51.08%
F1-Score given by SeqEval: 58.97%
Accuracy given by SeqEval: 97.78%
------------------------------------------------------------
Experiment: 32

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.75      0.63      0.69       238

   micro avg       0.75      0.63      0.69       238
   macro avg       0.75      0.63      0.69       238
weighted avg       0.75      0.63      0.69       238

Precision given by SeqEval: 74.75%
Recall given by SeqEval: 63.45%
F1-Score given by SeqEval: 68.64%
Accuracy given by SeqEval: 97.39%
------------------------------------------------------------
Experiment: 33

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.71      0.67      0.69        70

   micro avg       0.71      0.67      0.69        70
   macro avg       0.71      0.67      0.69        70
weighted avg       0.71      0.67      0.69        70

Precision given by SeqEval: 71.21%
Recall given by SeqEval: 67.14%
F1-Score given by SeqEval: 69.12%
Accuracy given by SeqEval: 99.29%
------------------------------------------------------------
Experiment: 34

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.51      0.51      0.51       245

   micro avg       0.51      0.51      0.51       245
   macro avg       0.51      0.51      0.51       245
weighted avg       0.51      0.51      0.51       245

Precision given by SeqEval: 50.82%
Recall given by SeqEval: 50.61%
F1-Score given by SeqEval: 50.72%
Accuracy given by SeqEval: 97.78%
------------------------------------------------------------
Experiment: 35

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.64      0.63      0.64       141

   micro avg       0.64      0.63      0.64       141
   macro avg       0.64      0.63      0.64       141
weighted avg       0.64      0.63      0.64       141

Precision given by SeqEval: 64.03%
Recall given by SeqEval: 63.12%
F1-Score given by SeqEval: 63.57%
Accuracy given by SeqEval: 98.76%
------------------------------------------------------------
Experiment: 36

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.51      0.68      0.58       207

   micro avg       0.51      0.68      0.58       207
   macro avg       0.51      0.68      0.58       207
weighted avg       0.51      0.68      0.58       207

Precision given by SeqEval: 50.90%
Recall given by SeqEval: 68.12%
F1-Score given by SeqEval: 58.26%
Accuracy given by SeqEval: 99.10%
------------------------------------------------------------
Experiment: 37

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.70      0.63      0.67       115

   micro avg       0.70      0.63      0.67       115
   macro avg       0.70      0.63      0.67       115
weighted avg       0.70      0.63      0.67       115

Precision given by SeqEval: 70.19%
Recall given by SeqEval: 63.48%
F1-Score given by SeqEval: 66.67%
Accuracy given by SeqEval: 99.21%
------------------------------------------------------------
Experiment: 38

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.58      0.67      0.62       136

   micro avg       0.58      0.67      0.62       136
   macro avg       0.58      0.67      0.62       136
weighted avg       0.58      0.67      0.62       136

Precision given by SeqEval: 58.33%
Recall given by SeqEval: 66.91%
F1-Score given by SeqEval: 62.33%
Accuracy given by SeqEval: 98.83%
------------------------------------------------------------
Experiment: 39

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.58      0.52      0.55       143

   micro avg       0.58      0.52      0.55       143
   macro avg       0.58      0.52      0.55       143
weighted avg       0.58      0.52      0.55       143

Precision given by SeqEval: 57.69%
Recall given by SeqEval: 52.45%
F1-Score given by SeqEval: 54.95%
Accuracy given by SeqEval: 98.78%
------------------------------------------------------------
Experiment: 40

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.50      0.59      0.54       120

   micro avg       0.50      0.59      0.54       120
   macro avg       0.50      0.59      0.54       120
weighted avg       0.50      0.59      0.54       120

Precision given by SeqEval: 50.00%
Recall given by SeqEval: 59.17%
F1-Score given by SeqEval: 54.20%
Accuracy given by SeqEval: 99.11%
------------------------------------------------------------
Experiment: 41

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.60      0.69      0.64       140

   micro avg       0.60      0.69      0.64       140
   macro avg       0.60      0.69      0.64       140
weighted avg       0.60      0.69      0.64       140

Precision given by SeqEval: 60.38%
Recall given by SeqEval: 68.57%
F1-Score given by SeqEval: 64.21%
Accuracy given by SeqEval: 98.44%
------------------------------------------------------------
Experiment: 42

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.70      0.77      0.73       163

   micro avg       0.70      0.77      0.73       163
   macro avg       0.70      0.77      0.73       163
weighted avg       0.70      0.77      0.73       163
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 08:03:48,789] [INFO] Set up nlp object from config
[2022-09-11 08:03:48,800] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 08:03:48,801] [INFO] Resuming training for: ['ner']
[2022-09-11 08:03:48,810] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 08:03:57,383] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 08:04:12,037] [INFO] Created vocabulary
[2022-09-11 08:04:13,552] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 08:04:15,204] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.07   60.27   56.75   64.27    0.60
 28     200           0.00  63051.15   61.45   63.99   59.10    0.61
 57     400           0.00  45762.30   64.86   67.98   62.02    0.65
 85     600           0.00  30793.83   63.83   67.33   60.67    0.64
114     800           0.00  21710.29   64.34   66.83   62.02    0.64
142    1000           0.00  15718.84   63.74   65.56   62.02    0.64
171    1200           0.00  11359.77   62.18   64.27   60.22    0.62
200    1400           0.00   8421.63   61.14   63.59   58.88    0.61
228    1600           0.00   6267.46   60.96   63.73   58.43    0.61
257    1800           0.00   4865.81   61.56   63.70   59.55    0.62
285    2000           0.00   3925.33   61.32   64.52   58.43    0.61
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 08:21:01,291] [INFO] Set up nlp object from config
[2022-09-11 08:21:01,301] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 08:21:01,301] [INFO] Resuming training for: ['ner']
[2022-09-11 08:21:01,310] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 08:21:09,901] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 08:21:24,492] [INFO] Created vocabulary
[2022-09-11 08:21:25,976] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 08:21:27,662] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    223.02   59.35   55.86   63.29    0.59
 28     200           0.00  63765.29   60.19   61.90   58.56    0.60
 57     400           0.00  44477.77   58.91   62.31   55.86    0.59
 85     600           0.00  29731.84   58.55   60.98   56.31    0.59
114     800           0.00  20780.71   58.33   60.00   56.76    0.58
142    1000           0.00  14689.98   57.54   59.33   55.86    0.58
171    1200           0.00  10494.08   58.30   60.19   56.53    0.58
200    1400           0.00   7450.31   57.86   59.02   56.76    0.58
228    1600           0.00   5638.00   57.47   59.19   55.86    0.57
257    1800           0.00   4427.83   57.76   58.56   56.98    0.58
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 08:37:36,359] [INFO] Set up nlp object from config
[2022-09-11 08:37:36,369] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 08:37:36,369] [INFO] Resuming training for: ['ner']
[2022-09-11 08:37:36,379] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 08:37:44,964] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 08:37:59,598] [INFO] Created vocabulary
[2022-09-11 08:38:01,099] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 08:38:02,749] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.52   65.81   61.25   71.09    0.66
 28     200           0.00  58337.88   68.67   68.82   68.52    0.69
 57     400           0.00  40442.22   69.13   70.20   68.09    0.69
 85     600           0.00  26405.52   68.68   69.28   68.09    0.69
114     800           0.00  18172.93   69.26   69.78   68.74    0.69
142    1000           0.00  12642.48   68.98   69.89   68.09    0.69
171    1200           0.00   8780.46   67.69   69.04   66.38    0.68
200    1400           0.00   6434.35   68.04   68.86   67.24    0.68
228    1600           0.00   4661.59   67.10   67.83   66.38    0.67
257    1800           0.00   3503.90   67.03   67.69   66.38    0.67
285    2000           0.00   2730.14   66.95   66.88   67.02    0.67
314    2200           0.00   2192.89   66.24   66.52   65.95    0.66
342    2400           0.00   1825.99   65.36   66.52   64.24    0.65
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 08:58:28,361] [INFO] Set up nlp object from config
[2022-09-11 08:58:28,371] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 08:58:28,371] [INFO] Resuming training for: ['ner']
[2022-09-11 08:58:28,380] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 08:58:36,965] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 08:58:51,598] [INFO] Created vocabulary
[2022-09-11 08:58:53,101] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 08:58:54,745] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.52   67.71   66.73   68.71    0.68
 28     200           0.00  53780.48   68.24   70.70   65.94    0.68
 57     400           0.00  37237.14   68.98   71.16   66.93    0.69
 85     600           0.00  24067.12   69.35   72.41   66.53    0.69
114     800           0.00  16070.15   68.44   70.68   66.34    0.68
142    1000           0.00  10879.37   69.38   71.34   67.52    0.69
171    1200           0.00   7398.25   69.26   71.76   66.93    0.69
200    1400           0.00   5225.66   69.12   71.46   66.93    0.69
228    1600           0.00   3840.11   68.45   71.15   65.94    0.68
257    1800           0.00   3032.65   68.57   70.74   66.53    0.69
285    2000           0.00   2378.70   68.91   71.01   66.93    0.69
314    2200           0.00   1915.51   68.59   71.46   65.94    0.69
342    2400           0.00   1595.46   68.79   71.43   66.34    0.69
371    2600           0.00   1373.78   68.67   71.86   65.74    0.69
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 09:20:59,781] [INFO] Set up nlp object from config
[2022-09-11 09:20:59,791] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 09:20:59,791] [INFO] Resuming training for: ['ner']
[2022-09-11 09:20:59,800] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 09:21:08,422] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 09:21:23,104] [INFO] Created vocabulary
[2022-09-11 09:21:24,607] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 09:21:26,213] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.80   57.05   55.18   59.05    0.57
 28     200           0.00  58773.66   56.81   59.02   54.75    0.57
 57     400           0.00  41820.74   55.80   57.91   53.85    0.56
 85     600           0.00  27693.90   55.53   57.84   53.39    0.56
114     800           0.00  19211.42   55.70   57.95   53.62    0.56
142    1000           0.00  13824.69   55.67   57.63   53.85    0.56
171    1200           0.00  10033.17   55.45   56.90   54.07    0.55
200    1400           0.00   7600.36   54.95   56.59   53.39    0.55
228    1600           0.00   5641.13   55.99   57.04   54.98    0.56
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 09:36:31,465] [INFO] Set up nlp object from config
[2022-09-11 09:36:31,475] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 09:36:31,475] [INFO] Resuming training for: ['ner']
[2022-09-11 09:36:31,484] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 09:36:40,005] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 09:36:54,647] [INFO] Created vocabulary
[2022-09-11 09:36:56,153] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 09:36:57,817] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00      0.01   54.08   51.82   56.55    0.54
 28     200           0.00  48095.55   53.50   54.66   52.38    0.53
 57     400           0.00  34569.81   52.89   54.04   51.79    0.53
 85     600           0.00  23006.01   54.21   55.51   52.98    0.54
114     800           0.00  15828.30   55.01   56.75   53.37    0.55
142    1000           0.00  11017.05   54.30   56.14   52.58    0.54
171    1200           0.00   7597.32   54.23   56.44   52.18    0.54
200    1400           0.00   5453.46   53.80   55.74   51.98    0.54
228    1600           0.00   3941.91   53.65   55.65   51.79    0.54
257    1800           0.00   3078.24   52.85   55.31   50.60    0.53
285    2000           0.00   2480.34   52.78   54.94   50.79    0.53
314    2200           0.00   1953.41   52.69   54.96   50.60    0.53
342    2400           0.00   1766.60   52.37   54.51   50.40    0.52
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 10:01:11,577] [INFO] Set up nlp object from config
[2022-09-11 10:01:11,586] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 10:01:11,587] [INFO] Resuming training for: ['ner']
[2022-09-11 10:01:11,596] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 10:01:20,032] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 10:01:34,404] [INFO] Created vocabulary
[2022-09-11 10:01:35,876] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 10:01:37,515] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00     95.47   65.53   61.69   69.87    0.66
 28     200           0.00  61912.59   66.28   66.73   65.83    0.66
 57     400           0.00  43687.60   66.08   66.93   65.26    0.66
 85     600           0.00  29726.60   66.67   67.32   66.03    0.67
114     800           0.00  20821.68   66.09   66.15   66.03    0.66
142    1000           0.00  15175.74   66.28   66.73   65.83    0.66
171    1200           0.00  11041.10   64.99   65.50   64.49    0.65
200    1400           0.00   8063.85   65.69   66.54   64.88    0.66
228    1600           0.00   5994.45   66.08   66.93   65.26    0.66
257    1800           0.00   4663.99   65.82   66.80   64.88    0.66
285    2000           0.00   3703.51   65.76   66.28   65.26    0.66
314    2200           0.00   2968.72   65.31   66.14   64.49    0.65
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_T.json[0m
[38;5;4mâ„¹ Auto-detected token-per-line NER format[0m
[38;5;3mâš  Document delimiters found, automatic document segmentation with `-n`
disabled.[0m
[38;5;3mâš  No sentence boundaries found. Use `-s` to automatically segment
sentences.[0m
[38;5;2mâœ” Generated output file (1 documents): gs66Spacy_V.json[0m
[38;5;2mâœ” Generated output file (52 documents): gs66Spacy_T.spacy[0m
[38;5;2mâœ” Generated output file (14 documents): gs66Spacy_V.spacy[0m
[2022-09-11 10:20:36,370] [INFO] Set up nlp object from config
[2022-09-11 10:20:36,379] [INFO] Pipeline: ['transformer', 'ner']
[2022-09-11 10:20:36,379] [INFO] Resuming training for: ['ner']
[2022-09-11 10:20:36,388] [INFO] Copying tokenizer from: ../cdssSciSpacy/model-best
[2022-09-11 10:20:44,873] [INFO] Copying vocab from: ../cdssSciSpacy/model-best
[2022-09-11 10:20:59,389] [INFO] Created vocabulary
[2022-09-11 10:21:00,871] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2022-09-11 10:21:02,470] [INFO] Initialized pipeline components: ['transformer']
[38;5;4mâ„¹ Saving to output directory: ../cdssSciSpacyGS66[0m
[38;5;4mâ„¹ Using GPU: 0[0m
[1m
=========================== Initializing pipeline ===========================[0m
[38;5;2mâœ” Initialized pipeline[0m
[1m
============================= Training pipeline =============================[0m
[38;5;4mâ„¹ Pipeline: ['transformer', 'ner'][0m
[38;5;4mâ„¹ Initial learn rate: 0.0[0m
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE 
---  ------  -------------  --------  ------  ------  ------  ------
  0       0           0.00    101.77   57.09   52.90   62.00    0.57
 28     200           0.00  53679.73   58.90   58.77   59.02    0.59
 57     400           0.00  37524.14   57.45   58.46   56.48    0.57
 85     600           0.00  25041.90   59.45   59.45   59.45    0.59
114     800           0.00  17005.36   58.77   59.61   57.96    0.59
142    1000           0.00  12069.94   59.59   60.17   59.02    0.60
171    1200           0.00   8584.75   59.15   59.28   59.02    0.59
200    1400           0.00   6160.70   59.98   60.30   59.66    0.60
228    1600           0.00   4537.85   60.40   60.08   60.72    0.60
257    1800           0.00   3723.43   59.51   59.57   59.45    0.60
285    2000           0.00   2961.23   59.51   59.57   59.45    0.60
314    2200           0.00   2301.48   59.51   60.22   58.81    0.60
342    2400           0.00   1916.77   60.08   60.08   60.08    0.60
371    2600           0.00   1726.75   59.19   59.57   58.81    0.59
400    2800           0.00   1473.50   59.64   59.83   59.45    0.60
428    3000           0.00   1189.18   59.98   60.30   59.66    0.60
457    3200           0.00   1025.98   59.51   59.57   59.45    0.60
[38;5;2mâœ” Saved pipeline to output directory[0m
../cdssSciSpacyGS66/model-last

Precision given by SeqEval: 70.00%
Recall given by SeqEval: 77.30%
F1-Score given by SeqEval: 73.47%
Accuracy given by SeqEval: 99.33%
------------------------------------------------------------
Experiment: 43

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.57      0.71      0.63       102

   micro avg       0.57      0.71      0.63       102
   macro avg       0.57      0.71      0.63       102
weighted avg       0.57      0.71      0.63       102

Precision given by SeqEval: 56.69%
Recall given by SeqEval: 70.59%
F1-Score given by SeqEval: 62.88%
Accuracy given by SeqEval: 99.16%
------------------------------------------------------------
Experiment: 44

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.63      0.58      0.61       209

   micro avg       0.63      0.58      0.61       209
   macro avg       0.63      0.58      0.61       209
weighted avg       0.63      0.58      0.61       209

Precision given by SeqEval: 63.21%
Recall given by SeqEval: 58.37%
F1-Score given by SeqEval: 60.70%
Accuracy given by SeqEval: 98.37%
------------------------------------------------------------
Experiment: 45

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.76      0.70      0.73       170

   micro avg       0.76      0.70      0.73       170
   macro avg       0.76      0.70      0.73       170
weighted avg       0.76      0.70      0.73       170

Precision given by SeqEval: 76.28%
Recall given by SeqEval: 70.00%
F1-Score given by SeqEval: 73.01%
Accuracy given by SeqEval: 99.58%
------------------------------------------------------------
Experiment: 46

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.58      0.63      0.60       156

   micro avg       0.58      0.63      0.60       156
   macro avg       0.58      0.63      0.60       156
weighted avg       0.58      0.63      0.60       156

Precision given by SeqEval: 57.99%
Recall given by SeqEval: 62.82%
F1-Score given by SeqEval: 60.31%
Accuracy given by SeqEval: 99.07%
------------------------------------------------------------
Experiment: 47

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.57      0.58      0.58       159

   micro avg       0.57      0.58      0.58       159
   macro avg       0.57      0.58      0.58       159
weighted avg       0.57      0.58      0.58       159

Precision given by SeqEval: 57.41%
Recall given by SeqEval: 58.49%
F1-Score given by SeqEval: 57.94%
Accuracy given by SeqEval: 98.98%
------------------------------------------------------------
Experiment: 48

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.71      0.53      0.61       100

   micro avg       0.71      0.53      0.61       100
   macro avg       0.71      0.53      0.61       100
weighted avg       0.71      0.53      0.61       100

Precision given by SeqEval: 70.67%
Recall given by SeqEval: 53.00%
F1-Score given by SeqEval: 60.57%
Accuracy given by SeqEval: 98.75%
------------------------------------------------------------
Experiment: 49

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.66      0.60      0.63       326

   micro avg       0.66      0.60      0.63       326
   macro avg       0.66      0.60      0.63       326
weighted avg       0.66      0.60      0.63       326

Precision given by SeqEval: 66.21%
Recall given by SeqEval: 59.51%
F1-Score given by SeqEval: 62.68%
Accuracy given by SeqEval: 97.90%
------------------------------------------------------------
Experiment: 50

SeqEval Metrics on sciSpacy + Synthetic 1866 Train / 622 Val + 66GS 52 Train + 14 Validation + 67GS Test:

              precision    recall  f1-score   support

          KP       0.66      0.41      0.51       516

   micro avg       0.66      0.41      0.51       516
   macro avg       0.66      0.41      0.51       516
weighted avg       0.66      0.41      0.51       516

Precision given by SeqEval: 66.46%
Recall given by SeqEval: 41.47%
F1-Score given by SeqEval: 51.07%
Accuracy given by SeqEval: 98.05%
------------------------------------------------------------
 
 
+------------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES REQUESTED | 
+------------------------------------------+ 
 
mem=62gb,walltime=17:00:00,ncpus=24
 
 
+-------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES USED | 
+-------------------------------------+ 
 
cput=18:19:26,mem=17192248kb,walltime=16:17:09,ncpus=24,cpupercent=112,vmem=49797140kb
 
 
